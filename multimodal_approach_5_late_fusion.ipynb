{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fujn61yoyqEn",
        "iRJNfewCyzKp",
        "1hsigKhMP_B_",
        "OHWcfxtfIYog",
        "shvP8OVMTLMA",
        "H0rfHvFo1pSM",
        "4m4mrRljTX8Z",
        "3MBCyKr3x3tJ",
        "Kc9bn93tpgWs",
        "k8a2zWRSXZiw",
        "s1TZcDa7TqNC",
        "SaniFeOOT3xK",
        "cRE6fOEDxcS0",
        "AWmxw9-ixcTR",
        "7kA7VZowxcTQ",
        "zmESGCm6A-93",
        "27DPww4p1kxg",
        "h5cD22opea7m",
        "hhWR97qH7Cz4",
        "l4PURWN-liZM",
        "98pZRJzglwVC",
        "92M2582nJsY8",
        "3mBpndOTKPt-",
        "l6BazDdmdl8V",
        "ifdjO4wdl4ZQ",
        "vzcUekWKdub4",
        "UWih6zSVd7YO",
        "H-PyQCSUu7pe",
        "vZEu2xoNKJ30",
        "GTxrzuyIpnXs",
        "oGAjLPikfdm6",
        "qwgqYIwvS0Ka"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e3a89da17fe64be591432ef19d0251ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fda86679d71942e6a0247b0c9801228e",
              "IPY_MODEL_e038bdab7ca742e89f7c16353215e764",
              "IPY_MODEL_51ca7bb0dbac447ba9a39b45578d1ff0"
            ],
            "layout": "IPY_MODEL_b00cb7f6370d4220abd74aabfc88b64e"
          }
        },
        "fda86679d71942e6a0247b0c9801228e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82b77457c10543778810817ee406d6c9",
            "placeholder": "​",
            "style": "IPY_MODEL_ed4d43577e2d4126808fdd19a29bdcea",
            "value": "100%"
          }
        },
        "e038bdab7ca742e89f7c16353215e764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_affab982548b4e8f835014f779fdfe73",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ede6774936e4cdb9f3d58b52e0ecaaf",
            "value": 100
          }
        },
        "51ca7bb0dbac447ba9a39b45578d1ff0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32b631450a554a518d9e10f350e42bac",
            "placeholder": "​",
            "style": "IPY_MODEL_61b3a04c7c3a487c9704b1474a8b1a41",
            "value": " 100/100 [06:21&lt;00:00,  3.73s/it]"
          }
        },
        "b00cb7f6370d4220abd74aabfc88b64e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82b77457c10543778810817ee406d6c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed4d43577e2d4126808fdd19a29bdcea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "affab982548b4e8f835014f779fdfe73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ede6774936e4cdb9f3d58b52e0ecaaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32b631450a554a518d9e10f350e42bac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61b3a04c7c3a487c9704b1474a8b1a41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NNRahat/ERMS/blob/main/multimodal_approach_5_late_fusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bismillahir Rahmanir Rahim"
      ],
      "metadata": {
        "id": "jXn8fKoLNwoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN model"
      ],
      "metadata": {
        "id": "fujn61yoyqEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class image(nn.Module):\n",
        "#   def __init__(self,\n",
        "#                input_shape:int,\n",
        "#                hidden_units:int,\n",
        "#                output_shape:int):\n",
        "#     super().__init__()\n",
        "#     self.conv_block_1 = nn.Sequential(\n",
        "#         nn.Conv2d(in_channels = input_shape,\n",
        "#                   out_channels = hidden_units,\n",
        "#                   padding = 1,\n",
        "#                   kernel_size = 3,\n",
        "#                   stride = 1),\n",
        "#         nn.ReLU(),\n",
        "#         nn.BatchNorm2d(num_features = hidden_units),\n",
        "#         nn.Conv2d(in_channels = hidden_units,\n",
        "#                   out_channels = hidden_units,\n",
        "#                   padding = 1,\n",
        "#                   kernel_size = 3,\n",
        "#                   stride = 1),\n",
        "#         nn.ReLU(),\n",
        "#         nn.BatchNorm2d(num_features = hidden_units),\n",
        "#         nn.MaxPool2d(kernel_size = 2),\n",
        "#     )\n",
        "#     self.conv_block_2 = nn.Sequential(\n",
        "#         nn.Conv2d(in_channels = hidden_units,\n",
        "#                   out_channels = hidden_units,\n",
        "#                   padding = 1,\n",
        "#                   kernel_size = 3,\n",
        "#                   stride = 1),\n",
        "#         nn.ReLU(),\n",
        "#         nn.BatchNorm2d(num_features = hidden_units),\n",
        "#         nn.Conv2d(in_channels = hidden_units,\n",
        "#                   out_channels = hidden_units,\n",
        "#                   padding = 1,\n",
        "#                   kernel_size = 3,\n",
        "#                   stride = 1),\n",
        "#         nn.ReLU(),\n",
        "#         nn.BatchNorm2d(num_features = hidden_units),\n",
        "#         nn.MaxPool2d(kernel_size = 2),\n",
        "#     )\n",
        "\n",
        "#     self.classifier = nn.Sequential(\n",
        "#         nn.Flatten(),\n",
        "#         nn.Linear(in_features = hidden_units*31*31,\n",
        "#                   out_features = 128),\n",
        "#         # nn.Dropout(0.1),\n",
        "#         nn.Linear(in_features = 128,\n",
        "#                   out_features = 64),\n",
        "#         # nn.Dropout(0.1),\n",
        "#         nn.Linear(in_features = 64,\n",
        "#                   out_features = output_shape)\n",
        "#     )\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     # x = self.conv_block_1(x)\n",
        "#     # print(x.shape)\n",
        "#     # x = self.conv_block_2(x)\n",
        "#     # print(x.shape)\n",
        "#     # x = self.classifier(x)\n",
        "#     # return x\n",
        "\n",
        "#     return self.classifier(self.conv_block_2(self.conv_block_1(x)))\n"
      ],
      "metadata": {
        "id": "DVG5dhldyKAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model creation"
      ],
      "metadata": {
        "id": "iRJNfewCyzKp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d3kdy1bP_B-"
      },
      "outputs": [],
      "source": [
        "# torch.manual_seed(42)\n",
        "# model_0 = TinyVGG(input_shape = 3,\n",
        "#                   hidden_units = 70,\n",
        "#                   output_shape = 1).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hsigKhMP_B_"
      },
      "source": [
        "### loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2RtQmOcP_B_"
      },
      "outputs": [],
      "source": [
        "# loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# optimizer = torch.optim.Adam(params = model_0.parameters(),\n",
        "#                              lr = 0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fe757e9578c7431e8ebc8f0fca6a265b",
            "1d88ad8679ae4bb8a56c6c572af204d3",
            "674e3d41118b4cfc8feb829a4d53c706",
            "01f5b45b0a414c82b18e67b009a5bafd",
            "1aba157239504a4cbeeb372978af05cf",
            "73676602f39d419d89f924e8b91b4f6b",
            "f19c019f4f614cc2acd0ae6b710ad271",
            "629520c3ae204c4782dd45e5dd30b9f2",
            "d25477e6b36b4da683e1089575f4e0d3",
            "95258a2beb1f47d086f1bf65b5e7c9b2",
            "42d8938018174144b9a92705dc972b68"
          ]
        },
        "outputId": "3211dfd2-40e1-4d3d-8afa-11fcb052f2c8",
        "id": "wzRnDU4hP_CA"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe757e9578c7431e8ebc8f0fca6a265b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "----------------\n",
            "train loss: 0.00019, train acc: 100.000% | test loss: 0.59090, test_acc: 95.295%\n",
            "\n",
            "Epoch: 2\n",
            "----------------\n",
            "train loss: 0.00008, train acc: 100.000% | test loss: 0.59796, test_acc: 95.556%\n",
            "\n",
            "Epoch: 3\n",
            "----------------\n",
            "train loss: 0.00007, train acc: 100.000% | test loss: 0.59231, test_acc: 95.556%\n",
            "\n",
            "Epoch: 4\n",
            "----------------\n",
            "train loss: 0.00006, train acc: 100.000% | test loss: 0.60925, test_acc: 95.556%\n",
            "\n",
            "Epoch: 5\n",
            "----------------\n",
            "train loss: 0.00005, train acc: 100.000% | test loss: 0.60212, test_acc: 95.556%\n",
            "\n",
            "Epoch: 6\n",
            "----------------\n",
            "train loss: 0.00005, train acc: 100.000% | test loss: 0.59918, test_acc: 95.165%\n",
            "\n",
            "Epoch: 7\n",
            "----------------\n",
            "train loss: 0.00004, train acc: 100.000% | test loss: 0.60192, test_acc: 95.425%\n",
            "\n",
            "Epoch: 8\n",
            "----------------\n",
            "train loss: 0.00004, train acc: 100.000% | test loss: 0.60045, test_acc: 95.425%\n",
            "\n",
            "Epoch: 9\n",
            "----------------\n",
            "train loss: 0.00003, train acc: 100.000% | test loss: 0.61846, test_acc: 95.425%\n",
            "\n",
            "Epoch: 10\n",
            "----------------\n",
            "train loss: 0.00003, train acc: 100.000% | test loss: 0.62682, test_acc: 95.295%\n",
            "\n",
            "Epoch: 11\n",
            "----------------\n",
            "train loss: 0.00003, train acc: 100.000% | test loss: 0.62466, test_acc: 95.425%\n",
            "\n",
            "Epoch: 12\n",
            "----------------\n",
            "train loss: 0.00003, train acc: 100.000% | test loss: 0.61800, test_acc: 95.425%\n",
            "\n",
            "Epoch: 13\n",
            "----------------\n",
            "train loss: 0.00002, train acc: 100.000% | test loss: 0.61262, test_acc: 95.425%\n",
            "\n",
            "Epoch: 14\n",
            "----------------\n",
            "train loss: 0.00002, train acc: 100.000% | test loss: 0.62722, test_acc: 95.425%\n",
            "\n",
            "Epoch: 15\n",
            "----------------\n",
            "train loss: 0.00002, train acc: 100.000% | test loss: 0.62099, test_acc: 95.686%\n",
            "\n",
            "Epoch: 16\n",
            "----------------\n",
            "train loss: 0.00002, train acc: 100.000% | test loss: 0.61609, test_acc: 95.686%\n",
            "\n",
            "Epoch: 17\n",
            "----------------\n",
            "train loss: 0.00001, train acc: 100.000% | test loss: 0.64079, test_acc: 95.425%\n",
            "\n",
            "Epoch: 18\n",
            "----------------\n",
            "train loss: 0.00001, train acc: 100.000% | test loss: 0.63759, test_acc: 95.556%\n",
            "\n",
            "Epoch: 19\n",
            "----------------\n",
            "train loss: 0.00002, train acc: 100.000% | test loss: 0.62400, test_acc: 95.686%\n",
            "\n",
            "Epoch: 20\n",
            "----------------\n",
            "train loss: 0.00001, train acc: 100.000% | test loss: 0.63690, test_acc: 95.556%\n"
          ]
        }
      ],
      "source": [
        "# # hyper parameters\n",
        "# EPOCHS = 50\n",
        "\n",
        "# model_12_results = binary_engine.Binary_engine(model = model_0,\n",
        "#                                 train_dataloader = train_dataloader,\n",
        "#                                 test_dataloader = test_dataloader,\n",
        "#                                 loss_fn = loss_fn,\n",
        "#                                 optimizer = optimizer,\n",
        "#                                 device = device,\n",
        "#                                 epochs = EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `knn impute` for missing values by `imputing` first"
      ],
      "metadata": {
        "id": "OHWcfxtfIYog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "categorical_features = [\"Sex\", \"Ethnicity\", \"Jaundice\", \"Family_mem_with_ASD\", \"Who completed the test\", \"contry_of_res\"]\n",
        "\n",
        "for feature in categorical_features:\n",
        "  data_without_target[feature] = label_encoder.fit_transform(data_without_target[feature])\n",
        "\n",
        "# Convert the DataFrame to a NumPy array\n",
        "data_array = data_without_target.values\n",
        "\n",
        "# Create a KNNImputer instance with K=3 (you can adjust K as needed)\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "\n",
        "# Fit and transform the dataset\n",
        "imputed_data_array = imputer.fit_transform(data_array)\n",
        "\n",
        "# Convert the imputed data back to a DataFrame (if needed)\n",
        "data_without_target = pd.DataFrame(imputed_data_array, columns=data_without_target.columns)\n"
      ],
      "metadata": {
        "id": "d7RkXg57ISH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_without_target.iloc[32]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVQ9xaItY6i5",
        "outputId": "a1025dac-1242-4305-e058-1fb23d9629d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "A1_Score                   1.0\n",
              "A2_Score                   0.0\n",
              "A3_Score                   0.0\n",
              "A4_Score                   1.0\n",
              "A5_Score                   0.0\n",
              "A6_Score                   1.0\n",
              "A7_Score                   1.0\n",
              "A8_Score                   1.0\n",
              "A9_Score                   1.0\n",
              "A10_Score                  1.0\n",
              "Qchat-10-Score             7.0\n",
              "age                        8.2\n",
              "Sex                        1.0\n",
              "Ethnicity                 10.0\n",
              "Jaundice                   0.0\n",
              "Family_mem_with_ASD        0.0\n",
              "contry_of_res             13.0\n",
              "Who completed the test     5.0\n",
              "Name: 32, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lTQJ5h2PLYDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding starts\n",
        "\n",
        "**Note:** <br>\n",
        "          Final Text size -> `302` <br>\n",
        "          Final Image size -> `3829`"
      ],
      "metadata": {
        "id": "shvP8OVMTLMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### how to download from google drive"
      ],
      "metadata": {
        "id": "H0rfHvFo1pSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1EAFajQrFaiz4ADhkqMw0YoHyLphnvVf_' -O Autism_Child_Dataset.csv\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1vhUhbv7vGvz6u7Oku5z-jlaQprf8lnk8' -O Toddler_Autism_dataset.csv\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1se7-h7kGZ38WmqsVzf6ZaQZDiaUNyv9q' -O toddler+child.csv\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1JZtkpfvORj6ab-NZvdz1onYDuDT0-kcJ' -O final_80_20_124res.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaZ7g7oW0fdO",
        "outputId": "faeff841-9d32-45b5-aeec-6d644a5549eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-06 16:16:37--  https://docs.google.com/uc?export=download&id=1EAFajQrFaiz4ADhkqMw0YoHyLphnvVf_\n",
            "Resolving docs.google.com (docs.google.com)... 173.194.203.101, 173.194.203.139, 173.194.203.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.203.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0c-90-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/armop6t96c0ld6j13kpfhvt5nfcrqbj1/1696608975000/15404988222846953523/*/1EAFajQrFaiz4ADhkqMw0YoHyLphnvVf_?e=download&uuid=c1168651-45e9-4743-9436-c2854b121b81 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-10-06 16:16:37--  https://doc-0c-90-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/armop6t96c0ld6j13kpfhvt5nfcrqbj1/1696608975000/15404988222846953523/*/1EAFajQrFaiz4ADhkqMw0YoHyLphnvVf_?e=download&uuid=c1168651-45e9-4743-9436-c2854b121b81\n",
            "Resolving doc-0c-90-docs.googleusercontent.com (doc-0c-90-docs.googleusercontent.com)... 74.125.20.132, 2607:f8b0:400e:c07::84\n",
            "Connecting to doc-0c-90-docs.googleusercontent.com (doc-0c-90-docs.googleusercontent.com)|74.125.20.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19085 (19K) [text/csv]\n",
            "Saving to: ‘Autism_Child_Dataset.csv’\n",
            "\n",
            "Autism_Child_Datase 100%[===================>]  18.64K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-10-06 16:16:37 (141 MB/s) - ‘Autism_Child_Dataset.csv’ saved [19085/19085]\n",
            "\n",
            "--2023-10-06 16:16:37--  https://docs.google.com/uc?export=download&id=1vhUhbv7vGvz6u7Oku5z-jlaQprf8lnk8\n",
            "Resolving docs.google.com (docs.google.com)... 173.194.203.101, 173.194.203.139, 173.194.203.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.203.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-90-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/du6avcohl5lu9in3legcecmhs6vtj9if/1696608975000/15404988222846953523/*/1vhUhbv7vGvz6u7Oku5z-jlaQprf8lnk8?e=download&uuid=d19d779b-b133-4ef6-918e-123e0e1259ac [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-10-06 16:16:38--  https://doc-08-90-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/du6avcohl5lu9in3legcecmhs6vtj9if/1696608975000/15404988222846953523/*/1vhUhbv7vGvz6u7Oku5z-jlaQprf8lnk8?e=download&uuid=d19d779b-b133-4ef6-918e-123e0e1259ac\n",
            "Resolving doc-08-90-docs.googleusercontent.com (doc-08-90-docs.googleusercontent.com)... 74.125.20.132, 2607:f8b0:400e:c07::84\n",
            "Connecting to doc-08-90-docs.googleusercontent.com (doc-08-90-docs.googleusercontent.com)|74.125.20.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67081 (66K) [text/csv]\n",
            "Saving to: ‘Toddler_Autism_dataset.csv’\n",
            "\n",
            "Toddler_Autism_data 100%[===================>]  65.51K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-10-06 16:16:38 (130 MB/s) - ‘Toddler_Autism_dataset.csv’ saved [67081/67081]\n",
            "\n",
            "--2023-10-06 16:16:38--  https://docs.google.com/uc?export=download&id=1se7-h7kGZ38WmqsVzf6ZaQZDiaUNyv9q\n",
            "Resolving docs.google.com (docs.google.com)... 173.194.203.101, 173.194.203.139, 173.194.203.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.203.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0k-90-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/it9e1t1l8ne4f3l81f0lmg433s8da0mr/1696608975000/15404988222846953523/*/1se7-h7kGZ38WmqsVzf6ZaQZDiaUNyv9q?e=download&uuid=c8969f81-a469-4197-b4c8-2296c661f086 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-10-06 16:16:39--  https://doc-0k-90-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/it9e1t1l8ne4f3l81f0lmg433s8da0mr/1696608975000/15404988222846953523/*/1se7-h7kGZ38WmqsVzf6ZaQZDiaUNyv9q?e=download&uuid=c8969f81-a469-4197-b4c8-2296c661f086\n",
            "Resolving doc-0k-90-docs.googleusercontent.com (doc-0k-90-docs.googleusercontent.com)... 74.125.20.132, 2607:f8b0:400e:c07::84\n",
            "Connecting to doc-0k-90-docs.googleusercontent.com (doc-0k-90-docs.googleusercontent.com)|74.125.20.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 83278 (81K) [text/csv]\n",
            "Saving to: ‘toddler+child.csv’\n",
            "\n",
            "toddler+child.csv   100%[===================>]  81.33K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-10-06 16:16:39 (90.4 MB/s) - ‘toddler+child.csv’ saved [83278/83278]\n",
            "\n",
            "--2023-10-06 16:16:39--  https://docs.google.com/uc?export=download&id=1JZtkpfvORj6ab-NZvdz1onYDuDT0-kcJ\n",
            "Resolving docs.google.com (docs.google.com)... 173.194.203.101, 173.194.203.139, 173.194.203.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.203.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-90-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2736ig1pid90oh82osciulfsid5ilj24/1696608975000/15404988222846953523/*/1JZtkpfvORj6ab-NZvdz1onYDuDT0-kcJ?e=download&uuid=e9733537-f0fd-4493-8550-f2144030c9d7 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-10-06 16:16:58--  https://doc-08-90-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2736ig1pid90oh82osciulfsid5ilj24/1696608975000/15404988222846953523/*/1JZtkpfvORj6ab-NZvdz1onYDuDT0-kcJ?e=download&uuid=e9733537-f0fd-4493-8550-f2144030c9d7\n",
            "Resolving doc-08-90-docs.googleusercontent.com (doc-08-90-docs.googleusercontent.com)... 74.125.20.132, 2607:f8b0:400e:c07::84\n",
            "Connecting to doc-08-90-docs.googleusercontent.com (doc-08-90-docs.googleusercontent.com)|74.125.20.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9533758 (9.1M) [application/x-zip-compressed]\n",
            "Saving to: ‘final_80_20_124res.zip’\n",
            "\n",
            "final_80_20_124res. 100%[===================>]   9.09M  31.6MB/s    in 0.3s    \n",
            "\n",
            "2023-10-06 16:16:59 (31.6 MB/s) - ‘final_80_20_124res.zip’ saved [9533758/9533758]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1JZtkpfvORj6ab-NZvdz1onYDuDT0-kcJ' -O final_80_20_124res.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6be36c22-a723-42ed-d51e-656f96b736f3",
        "id": "nBu-Dt_Z6d80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-06 16:16:59--  https://docs.google.com/uc?export=download&id=1JZtkpfvORj6ab-NZvdz1onYDuDT0-kcJ\n",
            "Resolving docs.google.com (docs.google.com)... 173.194.203.101, 173.194.203.139, 173.194.203.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.203.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-90-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2736ig1pid90oh82osciulfsid5ilj24/1696608975000/15404988222846953523/*/1JZtkpfvORj6ab-NZvdz1onYDuDT0-kcJ?e=download&uuid=72842700-d66c-4c1c-84c2-32156f5bfab8 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-10-06 16:17:28--  https://doc-08-90-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2736ig1pid90oh82osciulfsid5ilj24/1696608975000/15404988222846953523/*/1JZtkpfvORj6ab-NZvdz1onYDuDT0-kcJ?e=download&uuid=72842700-d66c-4c1c-84c2-32156f5bfab8\n",
            "Resolving doc-08-90-docs.googleusercontent.com (doc-08-90-docs.googleusercontent.com)... 74.125.20.132, 2607:f8b0:400e:c07::84\n",
            "Connecting to doc-08-90-docs.googleusercontent.com (doc-08-90-docs.googleusercontent.com)|74.125.20.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9533758 (9.1M) [application/x-zip-compressed]\n",
            "Saving to: ‘final_80_20_124res.zip’\n",
            "\n",
            "final_80_20_124res. 100%[===================>]   9.09M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2023-10-06 16:17:28 (119 MB/s) - ‘final_80_20_124res.zip’ saved [9533758/9533758]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### importing things"
      ],
      "metadata": {
        "id": "4m4mrRljTX8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn\n",
        "import os\n"
      ],
      "metadata": {
        "id": "q1-B1sYOTUBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CSV file"
      ],
      "metadata": {
        "id": "3MBCyKr3x3tJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### reading csv's to dataframe"
      ],
      "metadata": {
        "id": "Kc9bn93tpgWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('Autism_Child_Dataset.csv', encoding='utf-8')  # Load your patient data\n",
        "# toddler_data = pd.read_csv('Toddler_Autism_dataset.csv', encoding='utf-8')  # Load your patient data\n",
        "# child_toddler_data = pd.read_csv('toddler+child.csv', encoding='utf-8')  # Load your patient data"
      ],
      "metadata": {
        "id": "y1Sav6pKpfiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Separate features(X) and target(y) variable"
      ],
      "metadata": {
        "id": "k8a2zWRSXZiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Data Preprocessing for PyTorch\n",
        "# Separate features and target variable\n",
        "data_without_target = data.drop('Class', axis=1)\n",
        "\n",
        "# encoding targets for future\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder() # label_encoder is also used knn imputation\n",
        "\n",
        "# targets\n",
        "data['Class'] = label_encoder.fit_transform(data['Class'])\n",
        "y = data['Class'].values\n",
        "\n",
        "len(y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YdBePOMXXt-",
        "outputId": "260b87c7-9fa9-4548-c335-12ce8058760f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "292"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `MVI impute` for missing values"
      ],
      "metadata": {
        "id": "s1TZcDa7TqNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Data Cleaning\n",
        "# Handle missing values (replace with mean for numeric data)\n",
        "# Define the list of numeric features\n",
        "numeric_features = [\"A1_Score\", \"A2_Score\", \"A3_Score\", \"A4_Score\", \"A5_Score\", \"A6_Score\", \"A7_Score\", \"A8_Score\", \"A9_Score\", \"A10_Score\", \"Qchat-10-Score\", \"age\"]\n",
        "\n",
        "# Apply Mean Value Imputation (MVI) for numeric features\n",
        "for feature in numeric_features:\n",
        "  data_without_target[feature].fillna(data_without_target[feature].mean(), inplace=True)\n",
        "\n",
        "# # Handle missing values for categorical features with MVI\n",
        "categorical_features = [\"Sex\", \"Ethnicity\", \"Jaundice\", \"Family_mem_with_ASD\", \"Who completed the test\", \"contry_of_res\"]\n",
        "\n",
        "for feature in categorical_features:\n",
        "  data_without_target[feature].fillna(data_without_target[feature].mode()[0], inplace=True)"
      ],
      "metadata": {
        "id": "7Gdi13n7Tbtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `ONE HOTED ENCODING` to convert the strings into numerical values"
      ],
      "metadata": {
        "id": "SaniFeOOT3xK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Step 3: Feature Engineering\n",
        "# (You can add feature engineering steps here as needed)\n",
        "\n",
        "# One-hot encoding for categorical features\n",
        "data_without_target = pd.get_dummies(data_without_target, columns=categorical_features, drop_first=True)\n",
        "data_without_target.iloc[32]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHpeCbr2T0cp",
        "outputId": "b04f75df-b667-4bd7-c0d4-2b1f345f7a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "A1_Score                               1.0\n",
              "A2_Score                               0.0\n",
              "A3_Score                               0.0\n",
              "A4_Score                               1.0\n",
              "A5_Score                               0.0\n",
              "                                      ... \n",
              "contry_of_res_Turkey                   0.0\n",
              "contry_of_res_U.S. Outlying Islands    0.0\n",
              "contry_of_res_United Arab Emirates     0.0\n",
              "contry_of_res_United Kingdom           0.0\n",
              "contry_of_res_United States            0.0\n",
              "Name: 32, Length: 79, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### splitting `after` oversampling"
      ],
      "metadata": {
        "id": "cRE6fOEDxcS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `SMOTE` now we are oversampling the whole dataset"
      ],
      "metadata": {
        "id": "AWmxw9-ixcTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE  # Import SMOTE for oversampling\n",
        "# Initialize the SMOTE oversampler\n",
        "oversampler = SMOTE(sampling_strategy='minority', random_state=42)\n",
        "\n",
        "# assigning  `X` here cause i worked on data_without_targets till `One Hoted Encoding`\n",
        "X = data_without_target.values\n",
        "\n",
        "# Apply SMOTE for oversampling\n",
        "X_resampled, y_resampled = SMOTE(sampling_strategy='minority', random_state=42).fit_resample(X, y)\n",
        "\n",
        "#again Convert data to PyTorch tensors\n",
        "X_resampled = torch.tensor(X_resampled, dtype=torch.float32)\n",
        "y_resampled = torch.tensor(y_resampled, dtype=torch.float32)\n",
        "\n",
        "print(X_resampled)\n",
        "print(y_resampled)\n",
        "\n",
        "print(f'\\nX_train size: {len(X)} -> {len(X_resampled)}')\n",
        "print(f'y_train size: {len(y)} -> {len(y_resampled)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8388ac8-5639-43e3-d5ac-c0606163d467",
        "id": "twXT2bmpxcTS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000e+00, 1.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        [1.0000e+00, 1.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        [1.0000e+00, 1.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        ...,\n",
            "        [1.0000e+00, 0.0000e+00, 1.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        [1.0000e+00, 7.7877e-04, 1.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         1.0000e+00],\n",
            "        [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00]])\n",
            "tensor([0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
            "        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
            "\n",
            "X_train size: 292 -> 302\n",
            "y_train size: 292 -> 302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### split the dataset into train and test after oversampling"
      ],
      "metadata": {
        "id": "7kA7VZowxcTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## Split the resampled data into training and testing sets\n",
        "X_train_resampled, X_test, y_train_resampled, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "PXnMf0UoxcTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(y_train_resampled))\n",
        "print(len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0a21BDEK0n5",
        "outputId": "31bb687b-3c8a-4509-fea5-2ed51358efdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "241\n",
            "61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FS techniques"
      ],
      "metadata": {
        "id": "zmESGCm6A-93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "# from sklearn.preprocessing import QuantileTransformer, PowerTransformer, Normalizer, MaxAbsScaler\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the transformers\n",
        "qt = QuantileTransformer(n_quantiles=241, output_distribution='uniform')\n",
        "# pt = PowerTransformer(method='yeo-johnson')\n",
        "# normalizer = Normalizer(norm='l2')\n",
        "# mas = MaxAbsScaler()\n",
        "\n",
        "# Transform the features with each transformer\n",
        "X_train_qt = qt.fit_transform(X_train_resampled.numpy())\n",
        "X_test_qt = qt.transform(X_test.numpy())\n",
        "\n",
        "# X_train_pt = pt.fit(X_train_resampled.numpy())\n",
        "# X_train_pt = pt.transform(X_train_pt)\n",
        "# X_test_pt = pt.transform(X_test.numpy())\n",
        "\n",
        "# X_train_normalized = normalizer.transform(X_train_resampled.numpy())\n",
        "# X_test_normalized = normalizer.transform(X_test)\n",
        "\n",
        "# X_train_mas = mas.fit_transform(X_train_resampled.numpy())\n",
        "# X_test_mas = mas.transform(X_test.numpy())\n",
        "\n",
        "# Convert NumPy arrays to PyTorch tensors\n",
        "X_train_qt = torch.tensor(X_train_qt, dtype=torch.float32)\n",
        "X_test_qt = torch.tensor(X_test_qt, dtype=torch.float32)\n",
        "\n",
        "# X_train_normalized = torch.tensor(X_train_normalized, dtype=torch.float32)\n",
        "# X_test_normalized = torch.tensor(X_test_normalized, dtype=torch.float32)\n",
        "\n",
        "# X_train_mas = torch.tensor(X_train_mas, dtype=torch.float32)\n",
        "# X_test_mas = torch.tensor(X_test_mas, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "HgQNz5F4BED-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_qt, y_train_resampled\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLtSYEImIWvm",
        "outputId": "0de7ef16-1c55-4829-b38c-7ce71f0d5272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 1., 1.,  ..., 0., 1., 0.],\n",
              "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
              "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [1., 1., 1.,  ..., 0., 1., 0.],\n",
              "         [0., 1., 1.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 1., 0.]]),\n",
              " tensor([0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
              "         1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
              "         1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
              "         0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
              "         1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
              "         1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
              "         1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
              "         1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
              "         1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
              "         1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
              "         1., 1., 0., 0., 1., 1., 0.]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train and test dataset of text -> dataloader"
      ],
      "metadata": {
        "id": "27DPww4p1kxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text_data, classes):\n",
        "        self.text_data = text_data\n",
        "        self.classes = classes\n",
        "        self.shape = self.text_data.shape\n",
        "        self.size = self.text_data.size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text_data[idx]\n",
        "        label = self.classes[idx]\n",
        "        return text, label\n",
        "    # def repeat(self, first, second, third, fourth):\n",
        "    #     # Use the repeat function to change the shape of the tensor\n",
        "    #     self.text_data = self.text_data.repeat(first, second, third, fourth)\n",
        "\n",
        "# Create an instance of the custom dataset\n",
        "train_text = TextDataset(X_train_qt, y_train_resampled)\n",
        "test_text = TextDataset(X_test_qt, y_test)\n",
        "# X_train_resampled, X_test, y_train_resampled, y_test\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wSUu99uS0kqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# # Define your image data (just for reference)\n",
        "# image_data = torch.randn((30, 3, 124, 124))\n",
        "\n",
        "# # Get the height and width of your image data\n",
        "# image_height, image_width = image_data.size(2), image_data.size(3)\n",
        "\n",
        "# # Get the height and width of your text data\n",
        "# text_height, text_width = train_text.text_data.size(2), train_text.text_data.size(3)\n",
        "\n",
        "# # Calculate the amount of padding needed for both height and width\n",
        "# padding_height = image_height - text_height\n",
        "# padding_width = image_width - text_width\n",
        "\n",
        "# # Create a padding tuple for F.pad based on the calculated values\n",
        "# padding = (0, padding_width, 0, padding_height)\n",
        "\n",
        "# # Pad the text data to match the height and width of the image data\n",
        "# if padding_height > 0 or padding_width > 0:\n",
        "#     # Only pad if the text data is smaller than the image data in either dimension\n",
        "#     train_text.text_data = F.pad(train_text.text_data, padding, value=0)\n",
        "\n",
        "# # Now, train_text.text_data has the same height and width as your image data\n"
      ],
      "metadata": {
        "id": "gmyhTLoq1EAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text.text_data.shape"
      ],
      "metadata": {
        "id": "IDzW8M9NEi1b",
        "outputId": "3e5a60b7-4912-46bd-d199-581cafb4491a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([241, 79])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# # Desired shape [241, 3, 124, 124]\n",
        "# desired_shape = (241, 3, 124, 124)\n",
        "\n",
        "# # Calculate the padding required for each dimension\n",
        "# padding = [max(0, desired_shape[i] - train_text.text_data.shape[i]) for i in range(4)]\n",
        "\n",
        "# # Pad the tensor to match the desired shape\n",
        "# if any(padding):\n",
        "#     padding = [pad // 2 for pad in padding]  # Pad equally on both sides\n",
        "#     train_text.text_data = F.pad(train_text.text_data, padding, value=0)\n",
        "\n",
        "# # Now, tensor has the shape [241, 3, 124, 124]\n"
      ],
      "metadata": {
        "id": "PjINL6i2DW6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text.text_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rI1Kk9zN88JY",
        "outputId": "3cf6ed2c-0c3e-433a-f30b-1a719e5525f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([241, 79])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 30\n",
        "def train_test_text_dataloader(train_dataset,\n",
        "                               test_dataset,\n",
        "                               batch_size):\n",
        "  # Find the common values in the tensor\n",
        "  unique_values = set(train_dataset.classes.tolist())  # Convert the tensor to a list and create a set\n",
        "  class_names= [value for value in unique_values if train_dataset.classes.tolist().count(value) > 1]\n",
        "\n",
        "    # Turn images into data loaders\n",
        "  train_dataloader = DataLoader(\n",
        "      train_dataset,\n",
        "      batch_size=batch_size,\n",
        "      drop_last=True,\n",
        "      shuffle=True\n",
        "  )\n",
        "  test_dataloader = DataLoader(\n",
        "      test_dataset,\n",
        "      batch_size=batch_size,\n",
        "      drop_last=True,\n",
        "      shuffle=False\n",
        "  )\n",
        "\n",
        "  return train_dataloader, test_dataloader, class_names\n",
        "\n",
        "train_text_dataloader, test_text_dataloader, class_names = train_test_text_dataloader(train_text,test_text,batch_size)\n"
      ],
      "metadata": {
        "id": "yhYdqOWt1HI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5cD22opea7m"
      },
      "source": [
        "## image data_path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# k = 0\n",
        "# # for jiter in os.listdir(\"/content/final_80_20_124res\"):\n",
        "# for kiter in os.listdir(f\"/content/final_80_20_124res/test\"):\n",
        "#   for liter in os.listdir(f\"/content/final_80_20_124res/test/{kiter}\"):\n",
        "#     k = k + 1\n",
        "# print(k)"
      ],
      "metadata": {
        "id": "i8picgP-HGtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUVz4rrs5fQi",
        "outputId": "15010762-0eb9-422f-d13a-5142060008bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('final_80_20_124res')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "data_path = Path(\"final_80_20_124res\")\n",
        "data_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wHlQKVoeL0m"
      },
      "source": [
        "### extracting the .zip file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L82MNP0U0UlT"
      },
      "outputs": [],
      "source": [
        "if data_path.is_dir():\n",
        "  print(f\"directory {data_path} already exists\")\n",
        "else:\n",
        "  # unzip the file\n",
        "  import zipfile\n",
        "  zip_ref = zipfile.ZipFile('/content/final_80_20_124res.zip', 'r')\n",
        "  zip_ref.extractall('final_80_20_124res')\n",
        "  zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hwAhxOM0fVV",
        "outputId": "5f5731c3-7e3d-422a-d3b7-cb18ef584e81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autism_Child_Dataset.csv  final_80_20_124res.zip  Toddler_Autism_dataset.csv\n",
            "final_80_20_124res\t  sample_data\t\t  toddler+child.csv\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhWR97qH7Cz4"
      },
      "source": [
        "### train and test dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAN7A2zY4uCT"
      },
      "outputs": [],
      "source": [
        "train_dir = data_path / \"train\"\n",
        "test_dir = data_path / \"test\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### transform ToTensor()"
      ],
      "metadata": {
        "id": "l4PURWN-liZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_to_tensor = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "BXh8no1Nj2Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train and test dataloader"
      ],
      "metadata": {
        "id": "98pZRJzglwVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = datasets.ImageFolder(train_dir, transform=transform_to_tensor)\n",
        "test_data = datasets.ImageFolder(test_dir, transform=transform_to_tensor)\n",
        "num_workers = os.cpu_count()\n",
        "# Get class names\n",
        "class_names = train_data.classes\n",
        "\n",
        "# Turn images into data loaders\n",
        "train_dataloader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last= True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True,\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    test_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    drop_last= True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "d0DoiByfRqXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# label matching"
      ],
      "metadata": {
        "id": "CQz9tXG64zxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## label matching for Training `dont run twice`"
      ],
      "metadata": {
        "id": "92M2582nJsY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Assuming you have lists of image and text labels\n",
        "train_image_labels = list(train_data.targets)\n",
        "train_image_labels = [float(x) for x in train_image_labels]\n",
        "\n",
        "train_text_labels = list(train_text.classes)\n",
        "train_text_labels = [x.item() for x in train_text_labels]\n",
        "\n",
        "# Create a mapping from class labels to sample indices for both modalities\n",
        "train_image_labels_to_indices = {}\n",
        "train_text_labels_to_indices = {}\n",
        "\n",
        "for idx, label in enumerate(train_image_labels):\n",
        "    if label not in train_image_labels_to_indices:\n",
        "        train_image_labels_to_indices[label] = []\n",
        "    train_image_labels_to_indices[label].append(idx)\n",
        "\n",
        "for idx, label in enumerate(train_text_labels):\n",
        "    if label not in train_text_labels_to_indices:\n",
        "        train_text_labels_to_indices[label] = []\n",
        "    train_text_labels_to_indices[label].append(idx)"
      ],
      "metadata": {
        "id": "XnlghOFtKvJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform label matching by selecting samples with the same label from both modalities\n",
        "matched_samples_train = []\n",
        "print(train_image_labels_to_indices)\n",
        "# test_image_labels_to_indices = {}\n",
        "\n",
        "# train_text_labels_to_indices = {}\n",
        "# test_text_labels_to_indices = {}\n",
        "# Iterate through each unique label found in image_labels\n",
        "for label, image_indices in train_image_labels_to_indices.items():\n",
        "    # Check if the current label is also present in text_labels\n",
        "    if label in train_text_labels_to_indices:\n",
        "        # Get the text indices that have the same label as the current image label\n",
        "        text_indices = train_text_labels_to_indices[label]\n",
        "        # Calculate the number of samples to select for this label\n",
        "        # by taking the maximum of the counts in image_indices and text_indices\n",
        "        num_samples = max(len(image_indices), len(text_indices))\n",
        "        # Randomly shuffle the indices to ensure random selection\n",
        "        np.random.shuffle(image_indices)\n",
        "        np.random.shuffle(text_indices)\n",
        "\n",
        "        # Create text_indices by repeating text indices to match num_samples\n",
        "        # This ensures that there is one text sample for each image sample with the same label\n",
        "        text_indices = [text_indices[i % len(text_indices)] for i in range(num_samples)]\n",
        "\n",
        "        # Extend the matched_samples list with pairs of indices (image_idx, text_idx)\n",
        "        # These pairs correspond to samples with the same class label\n",
        "        matched_samples_train.extend([(image_idx, text_idx) for image_idx, text_idx in zip(image_indices, text_indices)])\n",
        "\n",
        "# matched_samples now contains pairs of indices (image_idx, text_idx) that have the same class label\n",
        "print(len(matched_samples_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWDJlYxGGAXC",
        "outputId": "77c947ac-70a1-408d-d317-1755f963ac1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0.0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215], 1.0: [1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062]}\n",
            "3063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## label matching for Testing `dont run twice`"
      ],
      "metadata": {
        "id": "3mBpndOTKPt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_labels = list(test_data.targets)\n",
        "test_image_labels = [float(x) for x in test_image_labels]\n",
        "\n",
        "test_text_labels = list(test_text.classes)\n",
        "test_text_labels = [x.item() for x in test_text_labels]\n",
        "\n",
        "test_image_labels_to_indices = {}\n",
        "test_text_labels_to_indices = {}\n",
        "\n",
        "for idx, label in enumerate(test_image_labels):\n",
        "    if label not in test_image_labels_to_indices:\n",
        "        test_image_labels_to_indices[label] = []\n",
        "    test_image_labels_to_indices[label].append(idx)\n",
        "\n",
        "for idx, label in enumerate(test_text_labels):\n",
        "    if label not in test_text_labels_to_indices:\n",
        "        test_text_labels_to_indices[label] = []\n",
        "    test_text_labels_to_indices[label].append(idx)"
      ],
      "metadata": {
        "id": "KdTmXAWsKzuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform label matching by selecting samples with the same label from both modalities\n",
        "matched_samples_test = []\n",
        "print(train_image_labels_to_indices)\n",
        "# test_image_labels_to_indices = {}\n",
        "\n",
        "# train_text_labels_to_indices = {}\n",
        "# test_text_labels_to_indices = {}\n",
        "# Iterate through each unique label found in image_labels\n",
        "for label, image_indices in test_image_labels_to_indices.items():\n",
        "    # Check if the current label is also present in text_labels\n",
        "    if label in test_text_labels_to_indices:\n",
        "        # Get the text indices that have the same label as the current image label\n",
        "        text_indices = test_text_labels_to_indices[label]\n",
        "        # Calculate the number of samples to select for this label\n",
        "        # by taking the maximum of the counts in image_indices and text_indices\n",
        "        num_samples = max(len(image_indices), len(text_indices))\n",
        "        # Randomly shuffle the indices to ensure random selection\n",
        "        np.random.shuffle(image_indices)\n",
        "        np.random.shuffle(text_indices)\n",
        "\n",
        "        # Create text_indices by repeating text indices to match num_samples\n",
        "        # This ensures that there is one text sample for each image sample with the same label\n",
        "        text_indices = [text_indices[i % len(text_indices)] for i in range(num_samples)]\n",
        "\n",
        "        # Extend the matched_samples list with pairs of indices (image_idx, text_idx)\n",
        "        # These pairs correspond to samples with the same class label\n",
        "        matched_samples_test.extend([(image_idx, text_idx) for image_idx, text_idx in zip(image_indices, text_indices)])\n",
        "\n",
        "# matched_samples now contains pairs of indices (image_idx, text_idx) that have the same class label\n",
        "print(len(matched_samples_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4895c369-d7e2-43a0-a9e3-dfea701a6efb",
        "id": "CcFNo3UqKPuW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0.0: [659, 1119, 94, 386, 732, 425, 969, 254, 12, 507, 584, 25, 1186, 142, 1053, 630, 854, 394, 156, 1183, 300, 683, 982, 7, 209, 81, 111, 550, 1211, 232, 15, 1157, 775, 724, 146, 92, 837, 56, 519, 662, 103, 478, 650, 792, 958, 161, 1172, 711, 924, 249, 272, 1201, 90, 719, 289, 309, 929, 646, 873, 1066, 110, 915, 912, 786, 65, 263, 1095, 1114, 826, 4, 759, 616, 277, 513, 957, 342, 1139, 1115, 45, 150, 1152, 357, 713, 727, 1173, 1126, 749, 333, 476, 1129, 212, 988, 1030, 735, 920, 354, 95, 1073, 1181, 704, 931, 702, 91, 80, 778, 410, 71, 502, 811, 1213, 1187, 782, 875, 809, 1208, 1080, 274, 829, 488, 531, 1131, 10, 1014, 986, 1171, 385, 341, 921, 923, 857, 634, 169, 606, 181, 9, 1094, 431, 472, 714, 751, 390, 799, 178, 467, 163, 203, 325, 487, 295, 689, 448, 885, 717, 1127, 222, 765, 16, 1117, 840, 264, 137, 832, 201, 38, 1071, 1203, 317, 83, 368, 779, 1015, 827, 503, 1177, 211, 200, 52, 479, 645, 706, 1174, 830, 532, 62, 304, 58, 35, 709, 338, 619, 889, 1075, 148, 319, 720, 205, 313, 496, 977, 1164, 170, 588, 666, 47, 493, 818, 936, 1009, 1156, 1035, 966, 63, 981, 396, 194, 546, 1077, 617, 708, 1122, 1088, 773, 1034, 154, 243, 681, 890, 176, 870, 511, 1142, 1026, 1204, 499, 1051, 145, 504, 861, 1096, 379, 972, 656, 913, 585, 597, 647, 864, 74, 962, 468, 1154, 956, 910, 314, 893, 1044, 196, 674, 640, 162, 322, 1024, 351, 651, 693, 528, 1206, 429, 310, 85, 544, 153, 1102, 123, 978, 869, 6, 40, 331, 67, 522, 365, 1193, 860, 491, 195, 198, 858, 1121, 324, 30, 787, 166, 1138, 815, 392, 573, 806, 838, 540, 688, 774, 853, 772, 177, 684, 231, 362, 416, 383, 521, 1025, 255, 344, 1144, 328, 813, 207, 949, 968, 339, 241, 976, 256, 1188, 691, 695, 603, 641, 754, 418, 21, 576, 262, 481, 376, 598, 430, 228, 536, 1215, 1150, 626, 134, 1175, 964, 637, 591, 39, 1047, 66, 235, 654, 172, 716, 622, 106, 859, 1067, 458, 932, 989, 922, 497, 267, 155, 1111, 449, 284, 991, 312, 1087, 180, 788, 963, 1060, 348, 878, 906, 973, 384, 595, 632, 917, 433, 1147, 397, 608, 288, 644, 131, 1042, 725, 349, 345, 578, 271, 1018, 571, 1027, 301, 523, 541, 538, 891, 959, 1059, 505, 876, 737, 817, 768, 628, 31, 851, 529, 265, 470, 257, 102, 643, 1043, 1062, 877, 484, 927, 1105, 441, 950, 1082, 297, 305, 311, 520, 581, 903, 439, 1198, 707, 182, 1048, 800, 904, 607, 615, 926, 1135, 633, 109, 307, 140, 886, 729, 938, 141, 928, 293, 261, 149, 897, 789, 553, 221, 1019, 120, 453, 909, 685, 398, 655, 587, 604, 753, 852, 237, 1134, 124, 577, 1003, 1016, 642, 306, 276, 971, 466, 967, 17, 229, 358, 767, 1155, 318, 191, 551, 104, 742, 567, 614, 700, 1028, 49, 561, 494, 474, 356, 882, 225, 108, 942, 179, 723, 286, 403, 184, 1133, 629, 1176, 250, 378, 37, 905, 445, 381, 1076, 298, 5, 743, 86, 558, 139, 409, 1197, 998, 1031, 865, 770, 3, 437, 79, 51, 107, 165, 589, 1050, 427, 896, 796, 1212, 48, 726, 649, 296, 1128, 572, 236, 527, 1103, 627, 415, 951, 251, 130, 50, 1038, 914, 703, 805, 1070, 329, 783, 1162, 515, 465, 907, 652, 823, 1210, 84, 11, 447, 747, 883, 710, 605, 623, 462, 945, 75, 676, 795, 452, 1046, 121, 215, 1032, 816, 542, 457, 582, 480, 114, 1194, 721, 671, 777, 776, 450, 624, 844, 291, 965, 1200, 1207, 802, 1130, 323, 1011, 72, 635, 1074, 780, 183, 1163, 663, 665, 125, 1036, 380, 374, 970, 539, 657, 454, 1072, 432, 738, 60, 1029, 292, 238, 881, 821, 895, 983, 993, 8, 46, 845, 718, 1008, 26, 260, 974, 850, 757, 428, 223, 961, 0, 697, 32, 933, 23, 686, 1191, 712, 1153, 1004, 87, 118, 1005, 1137, 1185, 791, 41, 580, 855, 282, 1002, 1081, 583, 512, 824, 482, 673, 694, 208, 115, 1120, 730, 1148, 463, 1033, 1112, 1109, 1118, 610, 1196, 935, 814, 822, 611, 273, 1146, 1100, 389, 579, 678, 566, 246, 387, 599, 128, 413, 930, 334, 143, 1065, 797, 639, 89, 455, 96, 14, 835, 407, 748, 880, 364, 1097, 1159, 217, 54, 55, 157, 648, 1116, 1140, 1040, 495, 28, 574, 801, 366, 785, 34, 839, 752, 158, 740, 197, 992, 590, 464, 804, 794, 975, 159, 766, 526, 240, 332, 93, 167, 1013, 224, 1007, 20, 849, 1108, 22, 27, 1190, 600, 872, 731, 781, 340, 1006, 1180, 1158, 1189, 327, 171, 57, 937, 625, 287, 336, 745, 1064, 308, 984, 393, 2, 997, 24, 828, 682, 419, 514, 784, 29, 898, 509, 842, 562, 954, 1202, 144, 244, 609, 475, 451, 1079, 560, 446, 705, 537, 315, 679, 756, 414, 486, 350, 370, 1113, 119, 395, 321, 841, 620, 1192, 1001, 204, 568, 1184, 1068, 985, 570, 220, 1069, 1106, 1132, 214, 947, 1170, 1165, 868, 136, 343, 1179, 423, 226, 1136, 100, 99, 892, 206, 621, 36, 469, 252, 1078, 798, 98, 459, 347, 846, 1083, 715, 733, 899, 638, 996, 1141, 741, 750, 417, 391, 862, 399, 586, 337, 879, 658, 744, 1110, 133, 534, 952, 1052, 761, 64, 280, 955, 270, 117, 888, 219, 1167, 793, 275, 948, 369, 999, 939, 127, 668, 953, 76, 285, 556, 129, 460, 601, 661, 564, 189, 278, 1049, 199, 1020, 88, 230, 173, 279, 1151, 987, 500, 677, 440, 352, 326, 764, 404, 1168, 61, 152, 863, 53, 473, 1099, 1205, 1058, 980, 866, 701, 602, 164, 266, 843, 59, 82, 126, 360, 871, 218, 1021, 692, 613, 900, 941, 101, 670, 113, 517, 477, 412, 594, 213, 884, 1023, 1000, 808, 283, 1037, 234, 216, 174, 186, 847, 960, 1086, 728, 675, 1012, 367, 535, 426, 138, 185, 97, 746, 834, 122, 1124, 422, 402, 1161, 70, 1209, 916, 73, 755, 722, 836, 353, 68, 810, 944, 188, 867, 874, 116, 147, 524, 1, 42, 33, 592, 485, 548, 202, 461, 112, 902, 490, 506, 372, 790, 1045, 233, 1090, 290, 925, 769, 848, 424, 294, 680, 193, 940, 335, 408, 192, 373, 253, 1017, 660, 13, 316, 259, 831, 442, 554, 355, 105, 245, 400, 934, 803, 1098, 248, 443, 669, 807, 525, 132, 401, 69, 911, 995, 559, 552, 268, 758, 498, 1195, 1123, 516, 1169, 1091, 43, 887, 1041, 1182, 151, 549, 1093, 530, 699, 760, 1085, 736, 696, 281, 258, 533, 636, 819, 919, 444, 555, 190, 1149, 979, 771, 618, 78, 436, 483, 918, 299, 894, 1166, 612, 565, 227, 856, 763, 187, 545, 175, 77, 303, 471, 405, 421, 1214, 543, 382, 1084, 596, 667, 593, 269, 239, 1101, 359, 375, 1057, 135, 569, 946, 434, 302, 1125, 435, 1107, 1143, 411, 510, 1054, 575, 388, 19, 1055, 406, 833, 557, 1089, 438, 812, 672, 1178, 508, 664, 631, 420, 363, 994, 739, 1160, 44, 1010, 547, 825, 501, 1056, 346, 1199, 210, 489, 168, 1061, 160, 456, 320, 943, 1063, 371, 734, 377, 242, 653, 1092, 563, 361, 762, 990, 820, 330, 901, 492, 687, 690, 1104, 247, 698, 18, 1145, 1022, 518, 908, 1039], 1.0: [2055, 2906, 2411, 2797, 1550, 1695, 2107, 2628, 2113, 2783, 1296, 1953, 2629, 1961, 1398, 1318, 1772, 1838, 2890, 1909, 2705, 2956, 2422, 2213, 2925, 1884, 2165, 1625, 1445, 1912, 2160, 2803, 1313, 1430, 2024, 1585, 2334, 2835, 2407, 1406, 2124, 1811, 2825, 2520, 1389, 1892, 1743, 1324, 1679, 2638, 1325, 1990, 2749, 2239, 1253, 1418, 2643, 2939, 2885, 2518, 2355, 1569, 1361, 1436, 2912, 2313, 2272, 2650, 1486, 1373, 2391, 2461, 2648, 1587, 1276, 2859, 2971, 1766, 1702, 2277, 2318, 2965, 2249, 2945, 1860, 2173, 2341, 1371, 2094, 2626, 2548, 2914, 1749, 1415, 2640, 1796, 2149, 2429, 2583, 1888, 2389, 1764, 2716, 1925, 2281, 2587, 1965, 2040, 2419, 2563, 1500, 1385, 2393, 2947, 1270, 1548, 2323, 2649, 2214, 1342, 2454, 2020, 2000, 1685, 1942, 1374, 1669, 1331, 1617, 2551, 1461, 2888, 3037, 2992, 1687, 1345, 2258, 1491, 1774, 1853, 2988, 1780, 1286, 2104, 1284, 2189, 2048, 1397, 2876, 1958, 2009, 2452, 1591, 2787, 2351, 2670, 1408, 1302, 2523, 2736, 1638, 2017, 2032, 1787, 2492, 2757, 1482, 2691, 2073, 2372, 2923, 2110, 2498, 2905, 1421, 1703, 2726, 2897, 1344, 2962, 2675, 2441, 2997, 1878, 1446, 1865, 2413, 2292, 1760, 2388, 1479, 2240, 2607, 1581, 2709, 2601, 1848, 1756, 2301, 1522, 1836, 2436, 2015, 2543, 1468, 1823, 2102, 1992, 2839, 2580, 2730, 2963, 1413, 2100, 1598, 2123, 1269, 1498, 1287, 2683, 1431, 2019, 2349, 2170, 3022, 1900, 2105, 2597, 2910, 2172, 2006, 1381, 2575, 1306, 2887, 2081, 1779, 1901, 1793, 2936, 1568, 2625, 1753, 1794, 1353, 1746, 2568, 1348, 2880, 2569, 2270, 2203, 2058, 1275, 1699, 2482, 1295, 1512, 2042, 2837, 1503, 2632, 2265, 2089, 1758, 2402, 2361, 2706, 2989, 2760, 1507, 2821, 2202, 2784, 2208, 1799, 2833, 1505, 1714, 2135, 2001, 1305, 1620, 2774, 1726, 2027, 1227, 1896, 2968, 2715, 1437, 1675, 1974, 2256, 2661, 1246, 1871, 1855, 1432, 1412, 1261, 2533, 1473, 1403, 1976, 1803, 2863, 2750, 2704, 2112, 2979, 1631, 2306, 2125, 1603, 3003, 2898, 2827, 1359, 1321, 2932, 1642, 3028, 2066, 2960, 1352, 2546, 1730, 1425, 2826, 1528, 1463, 2308, 2603, 2738, 2845, 2720, 2038, 3002, 2565, 1256, 2818, 2761, 2269, 1301, 2522, 2326, 1294, 1632, 1448, 2423, 3020, 2808, 2506, 2290, 2707, 1750, 1887, 2406, 2696, 1530, 2980, 2216, 2060, 2080, 1905, 1684, 2995, 2395, 2667, 1668, 1918, 2560, 2937, 2014, 2938, 2690, 2790, 2198, 2694, 2617, 2023, 1831, 1586, 2899, 2908, 2985, 2608, 2030, 1923, 2183, 2836, 1903, 1605, 2719, 1677, 2302, 2536, 2129, 2758, 2320, 2079, 1607, 2744, 2433, 2242, 2455, 1665, 3009, 2846, 3014, 1721, 1580, 2698, 2321, 2003, 1291, 2396, 2261, 2866, 2743, 2623, 1768, 2069, 2181, 2767, 2200, 1447, 2525, 1885, 1950, 1233, 2394, 1370, 2820, 1577, 1409, 2814, 1952, 2257, 1877, 1624, 1658, 1566, 2770, 2193, 2182, 1236, 1248, 2057, 2466, 1913, 3053, 2684, 2028, 2145, 2446, 1722, 3007, 1931, 1704, 2051, 2297, 1707, 2131, 2526, 1340, 2106, 1559, 2130, 2991, 3021, 2919, 1297, 1332, 2967, 1822, 1826, 1700, 2773, 2254, 1477, 2365, 1558, 2753, 3027, 2651, 1955, 2892, 1317, 2469, 2360, 1546, 2036, 2491, 1224, 2201, 1511, 1656, 1692, 2180, 2653, 2088, 1247, 2875, 1541, 2398, 2944, 3049, 1690, 1820, 1216, 2457, 2148, 2501, 2622, 2191, 1987, 3054, 2164, 1921, 2122, 1852, 1614, 1762, 1582, 1988, 2294, 1769, 2681, 1517, 2108, 2012, 1833, 1490, 1847, 2841, 2325, 1939, 2602, 1299, 2851, 2143, 1460, 2673, 2159, 2606, 2252, 1604, 2304, 2087, 2231, 1817, 2964, 2115, 2082, 1278, 1790, 2891, 1706, 1973, 2424, 2134, 1829, 1890, 2564, 1596, 2721, 2487, 2090, 1636, 1635, 2333, 1393, 2517, 1411, 1761, 2521, 2630, 2584, 2872, 1818, 2462, 2084, 2917, 2840, 1789, 1933, 2312, 2918, 1241, 2586, 1553, 1734, 2072, 2930, 1334, 1713, 2870, 1554, 1231, 2883, 1520, 1759, 1410, 3024, 1813, 1492, 2400, 2328, 1315, 2237, 1949, 2127, 1434, 2852, 1645, 1379, 1529, 2843, 2033, 2928, 1521, 1414, 2740, 1740, 3045, 1698, 1733, 1533, 1989, 2467, 2175, 2223, 3047, 1981, 2711, 1338, 2092, 2741, 1549, 1567, 2983, 1999, 2815, 1622, 1515, 1599, 2273, 2873, 1765, 2109, 1788, 2894, 1657, 2392, 1948, 2531, 2585, 2762, 2732, 1781, 1274, 1221, 1915, 1659, 2144, 2209, 1583, 1429, 1343, 2352, 1608, 1283, 1672, 2071, 1962, 1462, 2811, 2339, 2384, 2243, 2204, 2464, 1316, 1995, 1880, 2545, 2250, 1288, 1551, 2598, 2801, 2594, 3059, 2426, 1565, 1363, 2655, 2751, 1394, 2610, 2838, 1355, 3035, 2286, 1290, 1883, 2348, 2026, 1689, 1815, 1621, 1483, 2078, 2474, 2408, 2505, 1242, 2778, 2854, 2120, 1834, 2095, 2268, 2230, 1754, 1914, 1561, 2099, 2924, 2456, 1828, 2886, 2178, 1886, 3016, 2809, 2940, 3015, 1579, 1438, 2977, 2098, 2951, 2745, 1731, 1602, 2037, 1265, 1228, 1889, 2380, 1263, 2547, 2495, 3030, 2567, 2053, 2534, 1547, 2197, 1739, 2490, 2929, 2234, 2403, 1748, 1963, 1681, 2428, 1537, 1351, 1504, 1940, 1775, 2723, 2007, 1650, 2503, 2627, 1365, 2933, 1494, 2865, 1322, 2451, 3050, 1440, 1757, 1944, 1943, 1919, 1356, 2061, 1956, 1971, 1966, 1922, 2657, 1814, 2117, 1920, 2471, 2168, 3011, 1983, 2062, 1225, 2795, 2513, 1978, 2021, 2807, 1767, 1571, 1873, 1339, 1991, 2343, 2959, 1484, 2593, 2592, 1998, 2986, 2868, 1307, 1611, 2430, 1839, 2114, 1719, 2842, 2895, 2847, 1964, 3004, 1673, 2293, 2376, 2659, 2718, 2035, 1399, 2480, 1235, 2527, 2828, 2412, 2793, 1863, 1916, 1876, 2338, 2420, 1347, 1564, 2722, 1217, 1798, 2340, 2975, 1935, 1655, 2550, 2509, 1697, 1979, 2780, 2663, 1786, 2571, 2953, 2616, 2486, 2998, 1243, 1391, 2785, 2453, 2577, 2229, 1641, 1609, 1289, 1959, 3036, 2002, 1532, 1951, 3008, 2771, 2713, 2410, 1910, 2970, 2588, 2502, 1630, 2382, 3038, 2769, 3042, 1422, 1282, 1928, 2244, 1858, 2375, 1633, 2478, 1927, 2535, 2942, 2195, 2296, 2689, 2765, 2830, 1606, 2641, 2414, 2118, 1485, 1457, 1899, 1337, 2574, 2853, 2660, 2075, 1590, 1285, 2472, 1729, 2695, 2140, 1715, 2654, 1994, 2025, 2742, 1230, 2142, 1662, 2581, 1562, 1869, 1426, 1417, 1718, 2137, 2734, 2368, 2665, 2248, 1510, 2834, 2996, 2966, 1354, 2166, 1330, 1535, 2225, 2278, 1254, 1648, 2549, 2532, 1708, 2631, 2497, 1906, 2065, 2047, 1258, 1419, 2558, 1771, 2101, 1716, 2504, 2858, 2405, 1724, 1597, 1229, 2538, 2238, 3025, 1277, 2177, 2196, 1977, 2788, 1480, 1736, 2540, 1280, 2957, 2335, 1647, 3040, 1683, 2763, 2662, 2111, 2029, 2366, 2116, 2812, 1717, 1954, 1465, 1861, 2733, 2347, 1556, 2221, 2512, 2739, 2353, 2555, 2350, 2199, 1311, 1654, 1218, 3061, 1404, 2458, 1866, 1395, 2224, 2909, 1969, 1667, 2844, 1600, 2624, 1812, 1350, 2976, 1435, 2435, 1451, 2322, 1613, 1800, 2644, 1616, 2539, 2562, 2300, 2279, 2647, 1376, 1310, 2948, 2697, 1501, 2267, 2345, 3034, 2050, 2005, 1701, 1383, 2878, 2432, 1527, 1540, 1420, 2447, 2731, 2245, 2635, 2682, 2463, 2658, 2861, 2634, 2686, 1300, 2656, 2725, 1835, 1732, 2417, 1497, 2927, 1917, 2958, 1832, 1601, 2618, 1725, 1268, 2157, 2596, 2479, 2409, 1816, 2186, 1543, 1737, 1536, 2337, 2362, 2666, 2307, 1538, 1428, 1545, 1850, 1396, 2091, 1723, 2982, 2877, 2371, 1980, 1711, 2903, 1572, 2496, 2247, 1257, 2232, 1674, 2416, 2642, 2566, 2054, 1405, 2700, 1260, 2972, 2860, 2283, 1629, 2515, 1557, 2507, 2855, 1478, 1346, 2227, 1661, 2184, 2674, 1524, 1508, 1652, 1851, 1763, 2470, 2255, 2829, 1266, 1623, 1941, 2637, 1249, 1279, 3062, 2799, 1670, 2949, 1696, 1378, 2557, 1859, 1456, 3018, 2737, 2800, 1368, 1879, 1802, 2004, 2141, 3033, 2387, 2155, 1509, 1454, 3001, 2031, 1975, 2358, 2156, 2236, 2619, 2431, 2817, 2671, 2399, 1972, 2633, 2217, 2978, 1423, 2595, 2576, 1360, 1575, 2319, 1908, 2772, 1251, 1770, 1476, 1678, 1870, 2381, 2315, 2383, 2434, 2327, 2519, 2285, 2954, 1705, 1593, 2831, 3056, 1574, 2776, 2013, 2241, 2421, 2169, 1830, 2330, 3031, 2138, 2378, 2987, 1382, 2524, 1592, 1271, 2712, 2386, 1377, 2310, 1821, 2305, 2779, 2067, 1525, 1946, 1937, 2336, 1968, 2064, 2465, 1443, 1453, 1470, 2874, 1997, 2804, 1881, 1960, 1416, 1808, 2332, 1327, 2018, 1555, 2212, 2591, 2263, 2529, 1308, 2494, 2553, 1357, 1595, 3043, 1947, 2222, 1849, 2943, 2449, 2922, 1255, 1573, 2620, 1588, 2904, 1827, 2314, 1627, 2167, 2528, 2599, 2611, 2128, 1936, 1996, 2832, 1626, 1336, 1934, 2687, 2792, 2668, 1314, 1872, 1891, 1777, 2264, 2139, 2363, 2276, 1957, 1402, 2369, 1819, 2636, 2934, 2049, 1240, 1400, 2879, 2147, 2755, 2085, 1239, 2701, 2911, 1615, 2317, 2993, 2552, 2915, 2309, 2781, 2344, 1469, 1776, 1384, 2677, 1653, 2961, 2981, 2806, 2295, 1911, 2559, 2685, 2218, 1862, 1691, 3044, 1234, 2645, 1392, 1842, 2303, 1222, 2680, 2907, 2561, 2488, 2056, 2437, 1502, 1238, 1967, 1444, 1273, 1390, 2856, 2342, 2044, 2226, 2367, 2621, 1810, 1854, 2916, 1442, 1945, 2678, 1237, 2063, 1578, 1220, 2370, 2541, 2530, 2153, 1805, 1807, 2542, 2390, 3006, 1506, 2748, 1612, 2921, 2316, 1366, 1552, 2639, 2605, 1868, 2093, 2425, 2850, 2759, 2136, 2011, 1464, 1380, 2280, 2862, 2274, 1747, 2582, 3051, 1388, 1250, 2324, 1982, 2857, 1372, 2669, 3000, 1474, 2508, 1226, 2752, 2205, 2819, 1693, 2477, 1795, 1326, 2459, 1809, 2253, 3005, 1751, 1499, 1660, 1745, 2926, 1514, 1481, 2439, 2881, 2777, 1267, 1245, 2190, 2473, 1784, 2589, 1534, 1773, 2931, 2729, 1304, 2590, 2612, 2185, 3048, 2377, 2791, 2291, 2544, 2284, 2206, 2192, 2235, 2798, 2902, 2994, 2154, 2346, 2816, 1902, 1792, 2070, 1634, 1735, 2896, 2893, 1686, 2444, 1303, 2710, 2869, 1898, 1744, 3029, 2747, 1682, 1874, 2570, 2097, 2973, 1328, 1407, 2068, 1619, 1364, 1926, 1894, 2664, 2728, 3023, 2103, 1439, 1643, 1651, 2445, 2121, 2041, 2824, 2679, 2802, 1907, 2171, 2913, 2764, 2151, 1335, 1489, 2766, 2514, 1244, 1694, 2440, 2176, 1993, 1752, 2152, 2096, 3017, 2404, 2357, 2782, 1782, 2086, 2158, 2481, 2364, 1320, 2920, 2174, 1778, 2401, 1843, 1712, 2946, 2438, 1649, 2901, 1323, 2984, 2220, 2188, 2271, 2867, 2359, 2016, 1493, 1560, 1930, 1893, 3039, 1801, 3052, 2510, 2735, 1845, 2955, 2688, 2034, 1897, 1259, 2578, 2251, 1755, 1487, 1262, 1783, 2150, 1837, 1544, 2450, 1471, 1281, 2724, 2882, 1637, 1401, 1864, 1728, 2187, 1563, 3046, 1846, 2356, 2260, 1644, 1640, 3010, 1452, 2331, 1576, 2702, 1358, 1867, 2727, 2077, 1970, 1932, 2935, 1806, 2483, 2133, 2775, 2132, 2489, 3019, 3032, 1841, 2556, 2374, 1741, 2600, 2210, 1584, 2162, 1680, 2179, 2822, 1472, 1840, 1458, 2672, 1676, 1312, 2692, 1663, 2287, 2043, 1272, 2468, 1526, 2329, 1924, 1362, 2823, 1720, 2511, 2022, 1333, 1664, 2500, 2708, 2990, 2059, 1804, 2418, 2289, 1531, 1424, 2999, 1938, 1223, 2385, 1513, 2211, 1264, 2950, 2373, 1519, 1844, 2207, 2299, 1516, 2427, 2443, 3041, 2246, 2941, 2813, 1449, 2499, 2613, 2485, 1618, 1882, 2703, 2475, 1856, 2233, 2484, 1929, 1610, 1298, 2415, 2652, 2754, 1387, 2714, 2794, 1646, 2298, 2460, 1475, 2052, 1639, 2219, 1523, 1488, 2194, 1785, 1985, 2259, 2008, 2161, 1710, 2848, 2889, 2871, 1738, 2288, 2884, 1433, 2768, 2615, 2864, 1349, 2046, 2083, 1671, 1369, 2163, 2756, 3026, 2554, 1459, 3055, 3012, 2045, 2805, 2146, 1539, 2119, 1496, 2573, 2786, 2379, 2974, 3057, 1341, 1427, 2579, 2537, 1570, 2311, 2572, 1309, 3058, 1986, 2693, 1375, 2609, 1688, 1219, 2493, 2676, 2275, 2746, 2448, 2789, 1742, 2810, 1495, 1441, 1455, 2476, 2699, 1319, 2604, 1904, 1518, 1875, 1232, 1367, 1628, 2849, 1386, 1542, 2646, 2516, 1329, 1895, 1594, 1293, 2282, 2397, 2354, 1666, 2717, 2266, 3013, 1589, 2076, 2074, 2010, 1467, 2215, 1824, 2228, 2952, 1252, 1984, 1825, 3060, 1797, 2039, 2796, 1466, 2262, 1791, 1857, 2969, 2614, 1292, 2442, 2900, 1709, 1727, 2126, 1450]}\n",
            "766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multimodal dataset creation with `class`"
      ],
      "metadata": {
        "id": "emEOqfzpLllH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Transformation for images (modify as needed)\n",
        "# image_transform = transforms.Compose([\n",
        "#     transforms.ToTensor()\n",
        "# ])\n",
        "\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text_data, classes):\n",
        "        self.text_data = text_data\n",
        "        self.classes = classes\n",
        "        self.shape = self.text_data.shape\n",
        "        self.size = self.text_data.size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text_data[idx]\n",
        "        label = self.classes[idx]\n",
        "        return text, label\n",
        "    # def repeat(self, first, second, third, fourth):\n",
        "    #     # Use the repeat function to change the shape of the tensor\n",
        "    #     self.text_data = self.text_data.repeat(first, second, third, fourth)\n",
        "\n",
        "# Create an instance of the custom dataset\n",
        "train_text = TextDataset(X_train_qt, y_train_resampled)\n",
        "test_text = TextDataset(X_test_qt, y_test)\n",
        "# X_train_resampled, X_test, y_train_resampled, y_test\n",
        "\n",
        "\n",
        "\n",
        "# Create a custom multimodal dataset class\n",
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, image_dataset, text_dataset, matched_samples):\n",
        "        self.image_dataset = image_dataset\n",
        "        self.text_dataset = text_dataset\n",
        "        self.matched_samples = matched_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.matched_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_idx, text_idx = self.matched_samples[idx]\n",
        "        image_sample,_ = self.image_dataset[image_idx]\n",
        "        text_sample = self.text_dataset[text_idx]\n",
        "\n",
        "        # Get the targets (class labels) for the image and text samples\n",
        "        image_target = self.image_dataset.targets[image_idx]  # Assuming the image dataset has 'targets' attribute\n",
        "        text_target = self.text_dataset.classes[text_idx]  # Assuming the text dataset has 'targets' attribute\n",
        "\n",
        "        # Return a dictionary containing image, text, and targets\n",
        "        return {\n",
        "            'image': image_sample,\n",
        "            'text': text_sample,\n",
        "            'image_target': image_target,\n",
        "            'text_target': text_target\n",
        "        }\n",
        "\n",
        "\n",
        "# Create an instance of the multimodal dataset\n",
        "multimodal_train = MultimodalDataset(train_data, train_text, matched_samples_train)\n",
        "multimodal_test = MultimodalDataset(test_data, test_text, matched_samples_test)\n",
        "\n",
        "# Create a DataLoader for the multimodal dataset\n",
        "batch_size = 28\n",
        "multimodal_train_dataloader = DataLoader(multimodal_train, batch_size=batch_size, drop_last= True, shuffle=True)\n",
        "multimodal_test_dataloader = DataLoader(multimodal_test, batch_size=batch_size, drop_last= True, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "ArWUz0Z_LdpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## multimodal starts"
      ],
      "metadata": {
        "id": "Goa5ee3a0TKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "ZZRogHnLAVTd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "8903afca-42fd-4e18-b9e9-c5a32bfd4d16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###`CNN` model cls"
      ],
      "metadata": {
        "id": "l6BazDdmdl8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from PIL import Image\n",
        "\n",
        "# Load your text data from a CSV file\n",
        "# text_data = pd.read_csv('text_data.csv')\n",
        "# text_features = torch.tensor(text_data.values, dtype=torch.float32)\n",
        "\n",
        "\n",
        "# Define a CNN architecture for image processing\n",
        "class CNN2(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_shape:int,\n",
        "               hidden_units:int,\n",
        "               output_shape:int):\n",
        "    super().__init__()\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = input_shape,\n",
        "                  out_channels = hidden_units,\n",
        "                  padding = 1,\n",
        "                  kernel_size = 3,\n",
        "                  stride = 1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(num_features = hidden_units),\n",
        "        nn.Conv2d(in_channels = hidden_units,\n",
        "                  out_channels = hidden_units,\n",
        "                  padding = 1,\n",
        "                  kernel_size = 3,\n",
        "                  stride = 1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(num_features = hidden_units),\n",
        "        nn.MaxPool2d(kernel_size = 2),\n",
        "    )\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = hidden_units,\n",
        "                  out_channels = hidden_units,\n",
        "                  padding = 1,\n",
        "                  kernel_size = 3,\n",
        "                  stride = 1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(num_features = hidden_units),\n",
        "        nn.Conv2d(in_channels = hidden_units,\n",
        "                  out_channels = hidden_units,\n",
        "                  padding = 1,\n",
        "                  kernel_size = 3,\n",
        "                  stride = 1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(num_features = hidden_units),\n",
        "        nn.MaxPool2d(kernel_size = 2),\n",
        "    )\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features = hidden_units*31*31,\n",
        "                  out_features = 128),\n",
        "        # nn.Dropout(0.1),\n",
        "        nn.Linear(in_features = 128,\n",
        "                  out_features = 64),\n",
        "        # nn.Dropout(0.1),\n",
        "        nn.Linear(in_features = 64,\n",
        "                  out_features = output_shape),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x = self.conv_block_1(x)\n",
        "    # print(x.shape)\n",
        "    # x = self.conv_block_2(x)\n",
        "    # print(x.shape)\n",
        "    # x = self.classifier(x)\n",
        "    # return x\n",
        "\n",
        "    return self.classifier(self.conv_block_2(self.conv_block_1(x)))\n",
        "\n",
        "# # Create DataLoader for your image dataset (replace with your data loading logic)\n",
        "# image_transforms = transforms.Compose([transforms.ToTensor()])\n",
        "# image_dataset = YourImageDataset(image_data, labels, transform=image_transforms)\n",
        "# image_data_loader = DataLoader(image_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "ywe87tXveL54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2nd one"
      ],
      "metadata": {
        "id": "ifdjO4wdl4ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_shape:int,\n",
        "               hidden_units:int,\n",
        "               output_shape:int):\n",
        "    super().__init__()\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 3,\n",
        "                  out_channels = 32,\n",
        "                  padding = 0,\n",
        "                  kernel_size = 3,\n",
        "                  stride = 1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(num_features = 32),\n",
        "        # nn.MaxPool2d(kernel_size = 2),\n",
        "        nn.Conv2d(in_channels = 32,\n",
        "                  out_channels = 32,\n",
        "                  padding = 0,\n",
        "                  kernel_size = 3,\n",
        "                  stride = 1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(num_features = 32),\n",
        "        nn.MaxPool2d(kernel_size = 2)\n",
        "    )\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features = 32*60*60,\n",
        "                  out_features = 32),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.1),\n",
        "        # nn.Linear(in_features = 128,\n",
        "        #           out_features = 64),\n",
        "\n",
        "        nn.Linear(in_features = 32,\n",
        "                  out_features = 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x = self.conv_block_1(x)\n",
        "    # print(x.shape)\n",
        "    # x = self.conv_block_2(x)\n",
        "    # print(x.shape)\n",
        "    # x = self.classifier(x)\n",
        "    # return x\n",
        "\n",
        "    return self.classifier(self.conv_block_1(x))"
      ],
      "metadata": {
        "id": "5O7sf6acl2nG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `cnn_model_1` initiation"
      ],
      "metadata": {
        "id": "vzcUekWKdub4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model_4 = CNN(input_shape = 3,\n",
        "               hidden_units = 70,\n",
        "               output_shape = 1).to(device)"
      ],
      "metadata": {
        "id": "mCK4FEYdde79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `SVM` model initiation"
      ],
      "metadata": {
        "id": "UWih6zSVd7YO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Assuming labelsou have text labels (replace with your labels)\n",
        "# y = [...]  # List of corresponding labels\n",
        "from sklearn.svm import SVC\n",
        "# from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# # Train an SVM classifier for text\n",
        "# svm_classifier_4 = SVC(kernel='linear')\n",
        "svm_classifier_4 = SVC()\n",
        "\n",
        "\n",
        "# svm_classifier.fit(text, labels) ## fitting it in the training portion\n",
        "\n",
        "# Make predictions\n",
        "# y_pred_svm_QT = svm_classifier.predict(X_train_qt) ## will predict on the testing portion\n",
        "\n",
        "# print(y_pred_svm_QT.shape)\n",
        "# # Calculate accuracy and print classification report\n",
        "# accuracy_svm_QT = accuracy_score(y_train_resampled, y_pred_svm_QT)\n",
        "# report_svm_QT  = classification_report(y_train_resampled, y_pred_svm_QT)\n",
        "\n",
        "\n",
        "# print(\"SVM Accuracy QT:\", accuracy_svm_QT)\n",
        "# print(\"SVM Classification Report:\\n\", report_svm_QT)"
      ],
      "metadata": {
        "id": "TXZM6aTEOQsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class text_cnn(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_shape:int,\n",
        "               hidden_units:int,\n",
        "               output_shape:int):\n",
        "\n",
        "    super().__init__()\n",
        "    self.text_conv1 = nn.Sequential(\n",
        "        nn.Conv1d(in_channels = 79,\n",
        "                  out_channels = 32,\n",
        "                  kernel_size = 1,\n",
        "                  stride=1,\n",
        "                  padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm1d(num_features = 32),\n",
        "        nn.MaxPool1d(kernel_size = 1,\n",
        "                      stride=1,\n",
        "                      padding=0),\n",
        "        nn.Conv1d(in_channels = 32,\n",
        "                  out_channels = 20,\n",
        "                  kernel_size = 1,\n",
        "                  stride=1,\n",
        "                  padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm1d(num_features = 20),\n",
        "        nn.MaxPool1d(kernel_size = 1,\n",
        "                      stride=1,\n",
        "                      padding=0)\n",
        "    )\n",
        "    self.fc2 = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(20, 16),  # Match the text shape\n",
        "        # nn.ReLU(),\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(16, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    x = self.text_conv1(x)\n",
        "    # print(x.shape)\n",
        "    x = self.fc2(x)\n",
        "    # print(x.shape)\n",
        "\n",
        "    return x\n",
        "    # return self.fc2(self.text_conv1(x))"
      ],
      "metadata": {
        "id": "u5QwC4i3IN_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_text_model = text_cnn(input_shape = 79,\n",
        "               hidden_units = 70,\n",
        "               output_shape = 1).to(device)"
      ],
      "metadata": {
        "id": "o3Ff-mmAJC2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `Model creation` of multimodal"
      ],
      "metadata": {
        "id": "H-PyQCSUu7pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class multimodal(nn.Module):\n",
        "    def __init__(self, text_model, image_model):\n",
        "        super().__init__()\n",
        "        self.text_model = text_model\n",
        "        self.image_model = image_model\n",
        "\n",
        "\n",
        "        self.conv_1 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels = 2,\n",
        "                      out_channels = 32,\n",
        "                      kernel_size = 1,\n",
        "                      stride=1,\n",
        "                      padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size = 1,\n",
        "                         stride=1,\n",
        "                         padding=0)\n",
        "        )\n",
        "        self.classify_fusion = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2, 74),\n",
        "            # nn.ReLU(),\n",
        "            # nn.BatchNorm2d(8),\n",
        "            nn.Linear(74, 64),\n",
        "            # nn.ReLU(),\n",
        "            # nn.BatchNorm2d(4),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "    def forward(self, text_input, image_input):\n",
        "      text_output = self.text_model(text_input)\n",
        "      # print(x.shape)\n",
        "      image_output = self.image_model(image_input)\n",
        "\n",
        "      combined = torch.cat((text_output, image_output), dim = 1).unsqueeze(dim=0)\n",
        "      # combined = combined.permute(1,2,0)\n",
        "      x = self.conv_1(combined.permute(1,2,0))\n",
        "      # print(x.shape)\n",
        "      logit = self.classify_fusion(combined.permute(1,2,0))\n",
        "      # print(x.shape)\n",
        "      return logit\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IL0G4B71eg2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fourtin = multimodal(cnn_text_model, cnn_model_4).to(device)"
      ],
      "metadata": {
        "id": "1XF7NRBOYsE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### training"
      ],
      "metadata": {
        "id": "vZEu2xoNKJ30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in multimodal_train_dataloader:\n",
        "  # Extract image and text tensors from the batch\n",
        "  j = 0\n",
        "  images = batch['image']\n",
        "  texts = batch['text']\n",
        "  image_targets = batch['image_target']\n",
        "  text_targets = batch['text_target']\n",
        "  # for text, image in zip(text_targets,image_targets):\n",
        "  #   print(text)\n",
        "  #   print(image)\n",
        "  #   # print(image_targets)\n",
        "  #   j = j+1\n",
        "  print(texts)\n",
        "  print(j)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nf8SlP-LYZaI",
        "outputId": "2da46dcb-e896-46f3-d69a-330fa69b626d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[1., 1., 1.,  ..., 0., 0., 1.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 0., 1.,  ..., 0., 0., 1.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]]), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0.])]\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## without conv1d and also just normal 64 -> 32 outputs"
      ],
      "metadata": {
        "id": "GTxrzuyIpnXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the CNN for image processing\n",
        "num_epochs = 50\n",
        "\n",
        "optimizer = optim.Adam(cnn_model_4.parameters(), lr=0.001)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Define a function to train the image CNN\n",
        "# def train_cnn():\n",
        "num = 0\n",
        "flag = False\n",
        "# Training loop for CNN\n",
        "for epoch in range(num_epochs):\n",
        "  cnn_model_4.train()\n",
        "  train_loss = 0.0\n",
        "  train_acc = 0.0\n",
        "  total_acc = 0.0\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  accuracy_per_batch = 0\n",
        "  num_batch_run = 0\n",
        "  total_concat_loss = 0\n",
        "\n",
        "  for batch in multimodal_train_dataloader:\n",
        "    # predicted = []\n",
        "    total_batches = 0\n",
        "    # batch_size = 32\n",
        "\n",
        "    images = batch['image']\n",
        "    texts = batch['text']\n",
        "    image_labels = batch['image_target']\n",
        "    text_labels= batch['text_target']\n",
        "\n",
        "    images, image_labels= images.to(device), image_labels.to(device)\n",
        "\n",
        "    texts, _ = texts\n",
        "    texts, text_labels= texts.to(device), text_labels.to(device)\n",
        "\n",
        "\n",
        "    # datatype of labels is int64 so we have to convert it to float32\n",
        "    image_labels = image_labels.type(torch.float32)\n",
        "    # forward pass\n",
        "    train_logit = cnn_model_4(images).squeeze()\n",
        "    train_pred = torch.round(torch.sigmoid(train_logit))\n",
        "\n",
        "    # calculate the loss\n",
        "    loss = loss_fn(train_logit,image_labels)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    # calculate accuracy\n",
        "    acc = (((train_pred == image_labels).sum().item() / len(train_pred)) * 100 )\n",
        "    train_acc += acc\n",
        "\n",
        "    # optimizer zero_grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Train an SVM classifier for text\n",
        "    svm_classifier_4.fit(texts.cpu(), text_labels.cpu())\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred_svm_QT = svm_classifier_4.predict(texts.cpu())\n",
        "    y_pred_svm_QT = torch.tensor(y_pred_svm_QT)\n",
        "\n",
        "    combined_features = torch.cat((train_pred.to(device), y_pred_svm_QT.to(device)), dim=0)\n",
        "\n",
        "    if flag == False:\n",
        "      input_shape = len(combined_features)\n",
        "      # print(input_shape)\n",
        "      output_shape = input_shape // 2\n",
        "\n",
        "      fourtin = multimodal(input_shape, output_shape).to(device)\n",
        "      concat_optimizer = optim.Adam(fourtin.parameters(), lr=0.001)\n",
        "      flag = True\n",
        "\n",
        "    # print(combined_features.shape)\n",
        "    final_output = fourtin(combined_features)\n",
        "        # Binary prediction (0 or 1) based on the sigmoid output\n",
        "    predicted = (final_output > 0.5).float()\n",
        "\n",
        "    predicted = predicted.requires_grad_(True)\n",
        "\n",
        "\n",
        "    total = len(image_labels)\n",
        "    correct = (predicted == image_labels).sum().item()\n",
        "\n",
        "    acc = ((correct / total) * 100)\n",
        "    # print(acc)\n",
        "    total_acc += acc\n",
        "    num_batch_run += 1\n",
        "        # calculate the loss\n",
        "    concat_loss = loss_fn(predicted.squeeze(),image_labels)\n",
        "    total_concat_loss += concat_loss.item()\n",
        "\n",
        "    # optimizer zero_grad\n",
        "    concat_optimizer.zero_grad()\n",
        "\n",
        "    # loss backward\n",
        "    concat_loss.backward()\n",
        "\n",
        "    # optimizer step\n",
        "    concat_optimizer.step()\n",
        "\n",
        "  print(f\"--------------------Epoch: {epoch}----------------------------------\")\n",
        "  print(num_batch_run)\n",
        "  print(len(multimodal_train_dataloader))\n",
        "  total_acc = total_acc / len(multimodal_train_dataloader)\n",
        "  print(\"==============================\")\n",
        "  print(total_acc)\n",
        "\n",
        "  avg_loss = total_concat_loss / len(multimodal_train_dataloader)\n",
        "  print(avg_loss)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePrSPYWIfgxV",
        "outputId": "b1b3b75d-79de-4116-a652-2b8082080ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------Epoch: 0----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "51.67763157894737\n",
            "0.7053475059961017\n",
            "--------------------Epoch: 1----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "52.796052631578945\n",
            "0.6974049423870288\n",
            "--------------------Epoch: 2----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "51.51315789473684\n",
            "0.7065714196154945\n",
            "--------------------Epoch: 3----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "52.203947368421055\n",
            "0.7018926175017106\n",
            "--------------------Epoch: 4----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "50.69078947368421\n",
            "0.7111381072747079\n",
            "--------------------Epoch: 5----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "52.401315789473685\n",
            "0.7015508087057817\n",
            "--------------------Epoch: 6----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "52.56578947368421\n",
            "0.7009535965166593\n",
            "--------------------Epoch: 7----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "53.19078947368421\n",
            "0.6982227689341495\n",
            "--------------------Epoch: 8----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "53.05921052631579\n",
            "0.6980444939512955\n",
            "--------------------Epoch: 9----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "51.41447368421053\n",
            "0.7059392289111489\n",
            "--------------------Epoch: 10----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "52.69736842105263\n",
            "0.7010711911477541\n",
            "--------------------Epoch: 11----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "53.35526315789474\n",
            "0.6965578029030248\n",
            "--------------------Epoch: 12----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "52.33552631578947\n",
            "0.7001357636953655\n",
            "--------------------Epoch: 13----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "53.848684210526315\n",
            "0.6950784852630214\n",
            "--------------------Epoch: 14----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "51.8421052631579\n",
            "0.7045774999417757\n",
            "--------------------Epoch: 15----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "50.921052631578945\n",
            "0.7097818669519926\n",
            "--------------------Epoch: 16----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "52.56578947368421\n",
            "0.7009205115468878\n",
            "--------------------Epoch: 17----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "50.23026315789474\n",
            "0.714809841231296\n",
            "--------------------Epoch: 18----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "52.76315789473684\n",
            "0.7005125397130063\n",
            "--------------------Epoch: 19----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "51.48026315789474\n",
            "0.7073542714118958\n",
            "--------------------Epoch: 20----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "53.125\n",
            "0.6989468254541096\n",
            "--------------------Epoch: 21----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "52.171052631578945\n",
            "0.7032819396571109\n",
            "--------------------Epoch: 22----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "52.07236842105263\n",
            "0.7031238976277803\n",
            "--------------------Epoch: 23----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "53.28947368421053\n",
            "0.6983881824894955\n",
            "--------------------Epoch: 24----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "51.9078947368421\n",
            "0.7040445685386658\n",
            "--------------------Epoch: 25----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "52.4671052631579\n",
            "0.700551113956853\n",
            "--------------------Epoch: 26----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "54.276315789473685\n",
            "0.6918992343701814\n",
            "--------------------Epoch: 27----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "51.80921052631579\n",
            "0.7059208901304947\n",
            "--------------------Epoch: 28----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "51.2171052631579\n",
            "0.7079919413516396\n",
            "--------------------Epoch: 29----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "53.35526315789474\n",
            "0.6963538201231706\n",
            "--------------------Epoch: 30----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "51.80921052631579\n",
            "0.704794350423311\n",
            "--------------------Epoch: 31----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "52.4671052631579\n",
            "0.70062276877855\n",
            "--------------------Epoch: 32----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "53.7171052631579\n",
            "0.6933987793169524\n",
            "--------------------Epoch: 33----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "53.7171052631579\n",
            "0.6951170526052776\n",
            "--------------------Epoch: 34----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "54.078947368421055\n",
            "0.6951391678107413\n",
            "--------------------Epoch: 35----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "52.401315789473685\n",
            "0.701637199677919\n",
            "--------------------Epoch: 36----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "51.8421052631579\n",
            "0.7059208938949987\n",
            "--------------------Epoch: 37----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "52.56578947368421\n",
            "0.7019992382902848\n",
            "--------------------Epoch: 38----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "51.38157894736842\n",
            "0.7061303646940934\n",
            "--------------------Epoch: 39----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "53.51973684210526\n",
            "0.6953467519659745\n",
            "--------------------Epoch: 40----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "51.578947368421055\n",
            "0.7046748700894807\n",
            "--------------------Epoch: 41----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "51.74342105263158\n",
            "0.704860508441925\n",
            "--------------------Epoch: 42----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "50.49342105263158\n",
            "0.7106363748249255\n",
            "--------------------Epoch: 43----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "50.75657894736842\n",
            "0.7108348746048777\n",
            "--------------------Epoch: 44----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "52.5\n",
            "0.7011630679431714\n",
            "--------------------Epoch: 45----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "50.82236842105263\n",
            "0.710651117249539\n",
            "--------------------Epoch: 46----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "51.973684210526315\n",
            "0.7025174247591119\n",
            "--------------------Epoch: 47----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "52.43421052631579\n",
            "0.7018540457675332\n",
            "--------------------Epoch: 48----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "51.9078947368421\n",
            "0.7025578800000642\n",
            "--------------------Epoch: 49----------------------------------\n",
            "95\n",
            "95\n",
            "==============================\n",
            "51.11842105263158\n",
            "0.70795885826412\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## conv1d and shaping of the concated guy"
      ],
      "metadata": {
        "id": "DwEiK2ntpyn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# del fourtin, cnn_model_4, cnn_text_model"
      ],
      "metadata": {
        "id": "huhwRwHYbLGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "# concat_optimizer = optim.Adam(fourtin.parameters(), lr=0.001)\n",
        "optimizer = optim.Adam(fourtin.parameters(),\n",
        "                          lr=0.001)\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "results = {\n",
        "    \"train_loss\":[],\n",
        "    \"train_acc\":[],\n",
        "    \"test_loss\":[],\n",
        "    \"test_acc\":[]\n",
        "}\n",
        "\n",
        "# Training loop (adjust as needed)\n",
        "epochs = 100\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  fourtin.train()\n",
        "\n",
        "  train_loss = 0.0\n",
        "  train_acc = 0.0\n",
        "\n",
        "  for batch in multimodal_train_dataloader:\n",
        "\n",
        "    images = batch['image']\n",
        "    texts = batch['text']\n",
        "    image_labels = batch['image_target']\n",
        "    text_labels= batch['text_target']\n",
        "\n",
        "    images, image_labels= images.to(device), image_labels.to(device)\n",
        "\n",
        "    texts, _ = texts\n",
        "    texts, text_labels= texts.to(device).unsqueeze(dim = 0), text_labels.to(device) # unsqueeze it so i can have [1,32,79] from [32,79]\n",
        "\n",
        "    # datatype of labels is int64 so we have to convert it to float32\n",
        "    image_labels = image_labels.type(torch.float32)\n",
        "    # forward pass\n",
        "    # image_train_logit = cnn_model_4(images)\n",
        "    # # print(train_pred.shape)\n",
        "    # image_train_pred = torch.round(torch.sigmoid(image_train_logit))#.unsqueeze(dim = 1) #unsqueezed it so that i can concat in dim = 1\n",
        "\n",
        "    # # Train an SVM classifier for text\n",
        "    # # svm_classifier_4.fit(texts.cpu(), text_labels.cpu())\n",
        "\n",
        "    # # Make predictions\n",
        "    # text_train_logit = cnn_text_model(texts.permute(1,2,0))\n",
        "    # text_train_pred = torch.round(torch.sigmoid(text_train_logit))\n",
        "    # # print(text_train_pred.shape)\n",
        "    # # print(image_train_pred.shape)\n",
        "    # # break\n",
        "\n",
        "    # combined_features = torch.cat((texts, images), dim=1)\n",
        "\n",
        "    # combined_features = combined_features.unsqueeze(dim=0)\n",
        "\n",
        "    final_output = fourtin(texts.permute(1,2,0), images).squeeze()\n",
        "    # print(final_output)\n",
        "        # Binary prediction (0 or 1) based on the sigmoid output\n",
        "    predicted = (final_output > 0.5).float()\n",
        "\n",
        "    predicted = predicted.requires_grad_(True)\n",
        "\n",
        "\n",
        "    # calculate the loss\n",
        "    concat_loss = loss_fn(final_output,image_labels)\n",
        "    train_loss += concat_loss.item()\n",
        "\n",
        "\n",
        "    # calculate accuracy\n",
        "    acc = (((predicted == image_labels).sum().item() / len(image_labels)) * 100 )\n",
        "    train_acc += acc\n",
        "\n",
        "    # optimizer zero_grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # loss backward\n",
        "    concat_loss.backward()\n",
        "\n",
        "    # optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "  train_loss /= len(multimodal_train_dataloader)\n",
        "  train_acc /= len(multimodal_train_dataloader)\n",
        "                                  #############\n",
        "###################################  testing  ###########################################\n",
        "                                  #############\n",
        "\n",
        "  test_loss, test_acc = 0, 0\n",
        "  fourtin.eval()\n",
        "  cnn_model_4.eval()\n",
        "  cnn_text_model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for batch in multimodal_test_dataloader:\n",
        "    # Pass image and text inputs through the multi_model\n",
        "\n",
        "      images = batch['image']\n",
        "      texts = batch['text']\n",
        "      image_labels = batch['image_target']\n",
        "      text_labels= batch['text_target']\n",
        "\n",
        "      images, image_labels= images.to(device), image_labels.to(device)\n",
        "\n",
        "      texts, _ = texts\n",
        "      texts, text_labels= texts.to(device).unsqueeze(dim = 0), text_labels.to(device) # unsqueeze it so i can have [1,32,79] from [32,79]\n",
        "\n",
        "      # datatype of labels is int64 so we have to convert it to float32\n",
        "      image_labels = image_labels.type(torch.float32)\n",
        "\n",
        "      # # forward pass\n",
        "      # image_test_logit = cnn_model_4(images)\n",
        "      # # print(train_pred.shape)\n",
        "      # image_test_pred = torch.round(torch.sigmoid(image_test_logit)) #unsqueezed it so that i can concat in dim = 1\n",
        "\n",
        "      # Make `Text` predictions\n",
        "      # text_test_logit = cnn_text_model(texts.permute(1,2,0))\n",
        "      # text_test_pred = torch.round(torch.sigmoid(text_test_logit))\n",
        "      # print(image_test_pred.shape)\n",
        "      # print(text_test_logit.shape)\n",
        "\n",
        "      # combined_features = torch.cat((image_test_pred.to(device), text_test_pred.to(device)), dim=1)\n",
        "\n",
        "      # combined_features = combined_features.unsqueeze(dim=0) # unsqueezed it because myshape was [32,2] where it should be [32,2,1] so unsqueezed it permuted it.\n",
        "\n",
        "      con_test_logit = fourtin(texts.permute(1,2,0), images).squeeze()\n",
        "\n",
        "      con_test_pred = torch.round(torch.sigmoid(con_test_logit))\n",
        "\n",
        "      # calculate the loss\n",
        "      loss = loss_fn(con_test_logit, image_labels)\n",
        "      test_loss += loss.item()\n",
        "\n",
        "      # calculate the accuracy\n",
        "      acc = (((con_test_pred == image_labels).sum().item() / len(con_test_pred)) * 100)\n",
        "      test_acc += acc\n",
        "\n",
        "    test_loss /= len(multimodal_test_dataloader)\n",
        "    test_acc /= len(multimodal_test_dataloader)\n",
        "  # break\n",
        "  results[\"train_loss\"].append(train_loss)\n",
        "  results[\"train_acc\"].append(train_acc)\n",
        "  results[\"test_loss\"].append(test_loss)\n",
        "  results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "\n",
        "  print(f' ------------------Epoch: [{epoch+1}/{epochs}]-----------------')\n",
        "  print(f'Train_loss: {train_loss} - train_acc: {train_acc} - test_loss: {test_loss} - test_acc: {test_acc}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e3a89da17fe64be591432ef19d0251ac",
            "fda86679d71942e6a0247b0c9801228e",
            "e038bdab7ca742e89f7c16353215e764",
            "51ca7bb0dbac447ba9a39b45578d1ff0",
            "b00cb7f6370d4220abd74aabfc88b64e",
            "82b77457c10543778810817ee406d6c9",
            "ed4d43577e2d4126808fdd19a29bdcea",
            "affab982548b4e8f835014f779fdfe73",
            "8ede6774936e4cdb9f3d58b52e0ecaaf",
            "32b631450a554a518d9e10f350e42bac",
            "61b3a04c7c3a487c9704b1474a8b1a41"
          ]
        },
        "id": "hn9g0hSDpdx5",
        "outputId": "77c27ff6-7b70-4ab0-8658-67db42ce7980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3a89da17fe64be591432ef19d0251ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ------------------Epoch: [1/100]-----------------\n",
            "Train_loss: 0.5143381171270248 - train_acc: 90.13761467889911 - test_loss: 0.5030574787546087 - test_acc: 94.31216931216932\n",
            " ------------------Epoch: [2/100]-----------------\n",
            "Train_loss: 0.5045973057046943 - train_acc: 92.1035386631717 - test_loss: 0.5037221511205038 - test_acc: 88.09523809523809\n",
            " ------------------Epoch: [3/100]-----------------\n",
            "Train_loss: 0.4925149498729531 - train_acc: 94.72477064220183 - test_loss: 0.5120608398207912 - test_acc: 92.46031746031746\n",
            " ------------------Epoch: [4/100]-----------------\n",
            "Train_loss: 0.4911801492402313 - train_acc: 94.13499344691999 - test_loss: 0.4955195431356077 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [5/100]-----------------\n",
            "Train_loss: 0.4959396252391535 - train_acc: 93.61074705111402 - test_loss: 0.4986289695457176 - test_acc: 93.65079365079362\n",
            " ------------------Epoch: [6/100]-----------------\n",
            "Train_loss: 0.4996020299032194 - train_acc: 92.03800786369594 - test_loss: 0.4879745874139998 - test_acc: 90.74074074074072\n",
            " ------------------Epoch: [7/100]-----------------\n",
            "Train_loss: 0.4986832560202397 - train_acc: 91.97247706422019 - test_loss: 0.48863217344990484 - test_acc: 95.76719576719573\n",
            " ------------------Epoch: [8/100]-----------------\n",
            "Train_loss: 0.5023538363636086 - train_acc: 91.25163826998687 - test_loss: 0.4966720872455173 - test_acc: 93.65079365079362\n",
            " ------------------Epoch: [9/100]-----------------\n",
            "Train_loss: 0.5105509643160969 - train_acc: 89.67889908256886 - test_loss: 0.5047119866918635 - test_acc: 91.53439153439157\n",
            " ------------------Epoch: [10/100]-----------------\n",
            "Train_loss: 0.5096677992868861 - train_acc: 90.46526867627784 - test_loss: 0.527997139427397 - test_acc: 83.2010582010582\n",
            " ------------------Epoch: [11/100]-----------------\n",
            "Train_loss: 0.5089790722645751 - train_acc: 90.49803407601574 - test_loss: 0.5122561432697155 - test_acc: 89.55026455026457\n",
            " ------------------Epoch: [12/100]-----------------\n",
            "Train_loss: 0.5092704689830815 - train_acc: 89.54783748361731 - test_loss: 0.5037305123276181 - test_acc: 58.06878306878307\n",
            " ------------------Epoch: [13/100]-----------------\n",
            "Train_loss: 0.5108540101882515 - train_acc: 91.18610747051115 - test_loss: 0.5199646188153161 - test_acc: 91.5343915343915\n",
            " ------------------Epoch: [14/100]-----------------\n",
            "Train_loss: 0.5089688106961207 - train_acc: 90.59633027522932 - test_loss: 0.4881372429706432 - test_acc: 97.48677248677248\n",
            " ------------------Epoch: [15/100]-----------------\n",
            "Train_loss: 0.5045616943901832 - train_acc: 91.80865006553081 - test_loss: 0.4881372429706432 - test_acc: 97.48677248677248\n",
            " ------------------Epoch: [16/100]-----------------\n",
            "Train_loss: 0.501457698028022 - train_acc: 92.20183486238531 - test_loss: 0.4963397372651983 - test_acc: 96.16402116402116\n",
            " ------------------Epoch: [17/100]-----------------\n",
            "Train_loss: 0.49669491151057255 - train_acc: 93.25032765399737 - test_loss: 0.4881372429706432 - test_acc: 97.48677248677248\n",
            " ------------------Epoch: [18/100]-----------------\n",
            "Train_loss: 0.5160902864342436 - train_acc: 87.87680209698559 - test_loss: 0.511924699500755 - test_acc: 93.65079365079364\n",
            " ------------------Epoch: [19/100]-----------------\n",
            "Train_loss: 0.5176874311692125 - train_acc: 88.5976408912189 - test_loss: 0.5198019720889904 - test_acc: 90.74074074074072\n",
            " ------------------Epoch: [20/100]-----------------\n",
            "Train_loss: 0.5278753651938307 - train_acc: 85.91087811271296 - test_loss: 0.527339376785137 - test_acc: 88.75661375661373\n",
            " ------------------Epoch: [21/100]-----------------\n",
            "Train_loss: 0.5276622624572264 - train_acc: 86.33682830930539 - test_loss: 0.52800452709198 - test_acc: 89.4179894179894\n",
            " ------------------Epoch: [22/100]-----------------\n",
            "Train_loss: 0.5237078456156844 - train_acc: 88.1389252948886 - test_loss: 0.5198019720889904 - test_acc: 90.74074074074072\n",
            " ------------------Epoch: [23/100]-----------------\n",
            "Train_loss: 0.5321221001651308 - train_acc: 86.99213630406291 - test_loss: 0.5513022365393462 - test_acc: 58.06878306878307\n",
            " ------------------Epoch: [24/100]-----------------\n",
            "Train_loss: 0.5386612453591932 - train_acc: 87.22149410222801 - test_loss: 0.535712323806904 - test_acc: 58.06878306878307\n",
            " ------------------Epoch: [25/100]-----------------\n",
            "Train_loss: 0.5388297643683372 - train_acc: 87.25425950196593 - test_loss: 0.5439013816692211 - test_acc: 58.06878306878307\n",
            " ------------------Epoch: [26/100]-----------------\n",
            "Train_loss: 0.5343254069122699 - train_acc: 87.94233289646138 - test_loss: 0.5357121648611846 - test_acc: 89.81481481481481\n",
            " ------------------Epoch: [27/100]-----------------\n",
            "Train_loss: 0.5288406306997352 - train_acc: 88.85976408912191 - test_loss: 0.5437520797605868 - test_acc: 87.69841269841267\n",
            " ------------------Epoch: [28/100]-----------------\n",
            "Train_loss: 0.5329299823406639 - train_acc: 88.04062909567499 - test_loss: 0.5437520797605868 - test_acc: 87.69841269841267\n",
            " ------------------Epoch: [29/100]-----------------\n",
            "Train_loss: 0.5293746705448955 - train_acc: 88.6959370904325 - test_loss: 0.5357121648611846 - test_acc: 89.81481481481481\n",
            " ------------------Epoch: [30/100]-----------------\n",
            "Train_loss: 0.5284858417073521 - train_acc: 88.66317169069461 - test_loss: 0.5437520797605868 - test_acc: 87.69841269841267\n",
            " ------------------Epoch: [31/100]-----------------\n",
            "Train_loss: 0.5288406553071573 - train_acc: 88.85976408912191 - test_loss: 0.5363697553122485 - test_acc: 88.88888888888886\n",
            " ------------------Epoch: [32/100]-----------------\n",
            "Train_loss: 0.5154646749890178 - train_acc: 90.20314547837488 - test_loss: 0.4961771578700454 - test_acc: 95.37037037037037\n",
            " ------------------Epoch: [33/100]-----------------\n",
            "Train_loss: 0.5073184606132157 - train_acc: 91.12057667103537 - test_loss: 0.4961771578700454 - test_acc: 95.37037037037037\n",
            " ------------------Epoch: [34/100]-----------------\n",
            "Train_loss: 0.5051557027965511 - train_acc: 91.05504587155963 - test_loss: 0.4961771578700454 - test_acc: 95.37037037037037\n",
            " ------------------Epoch: [35/100]-----------------\n",
            "Train_loss: 0.5061298297085893 - train_acc: 91.15334207077329 - test_loss: 0.4961771578700454 - test_acc: 95.37037037037037\n",
            " ------------------Epoch: [36/100]-----------------\n",
            "Train_loss: 0.5000293364218615 - train_acc: 92.2346002621232 - test_loss: 0.4961771578700454 - test_acc: 95.37037037037037\n",
            " ------------------Epoch: [37/100]-----------------\n",
            "Train_loss: 0.5013513061978402 - train_acc: 92.00524246395804 - test_loss: 0.4961771578700454 - test_acc: 95.37037037037037\n",
            " ------------------Epoch: [38/100]-----------------\n",
            "Train_loss: 0.5015225014008513 - train_acc: 91.9724770642202 - test_loss: 0.4961771578700454 - test_acc: 95.37037037037037\n",
            " ------------------Epoch: [39/100]-----------------\n",
            "Train_loss: 0.49952922146254725 - train_acc: 92.69331585845352 - test_loss: 0.495519549758346 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [40/100]-----------------\n",
            "Train_loss: 0.5019015513975685 - train_acc: 92.85714285714283 - test_loss: 0.495519549758346 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [41/100]-----------------\n",
            "Train_loss: 0.49968301292953143 - train_acc: 93.34862385321104 - test_loss: 0.5029018697915254 - test_acc: 95.10582010582011\n",
            " ------------------Epoch: [42/100]-----------------\n",
            "Train_loss: 0.5046546642386586 - train_acc: 92.52948885976407 - test_loss: 0.5029018697915254 - test_acc: 95.10582010582011\n",
            " ------------------Epoch: [43/100]-----------------\n",
            "Train_loss: 0.5053153546578294 - train_acc: 92.16906946264747 - test_loss: 0.5029018697915254 - test_acc: 95.10582010582011\n",
            " ------------------Epoch: [44/100]-----------------\n",
            "Train_loss: 0.5024781292731609 - train_acc: 92.82437745740498 - test_loss: 0.5029018697915254 - test_acc: 95.10582010582011\n",
            " ------------------Epoch: [45/100]-----------------\n",
            "Train_loss: 0.5062068082870693 - train_acc: 91.9724770642202 - test_loss: 0.5104393594794803 - test_acc: 95.10582010582011\n",
            " ------------------Epoch: [46/100]-----------------\n",
            "Train_loss: 0.5041951301994674 - train_acc: 92.49672346002623 - test_loss: 0.5029018697915254 - test_acc: 95.10582010582011\n",
            " ------------------Epoch: [47/100]-----------------\n",
            "Train_loss: 0.5045886977550087 - train_acc: 92.5294888597641 - test_loss: 0.5029018697915254 - test_acc: 95.10582010582011\n",
            " ------------------Epoch: [48/100]-----------------\n",
            "Train_loss: 0.500794129907538 - train_acc: 93.15203145478375 - test_loss: 0.5029018697915254 - test_acc: 95.10582010582011\n",
            " ------------------Epoch: [49/100]-----------------\n",
            "Train_loss: 0.5022951085633094 - train_acc: 92.85714285714285 - test_loss: 0.5029018697915254 - test_acc: 95.10582010582011\n",
            " ------------------Epoch: [50/100]-----------------\n",
            "Train_loss: 0.5014036672377805 - train_acc: 93.086500655308 - test_loss: 0.5029018697915254 - test_acc: 95.10582010582011\n",
            " ------------------Epoch: [51/100]-----------------\n",
            "Train_loss: 0.5031334640236076 - train_acc: 92.69331585845346 - test_loss: 0.5029018697915254 - test_acc: 95.10582010582011\n",
            " ------------------Epoch: [52/100]-----------------\n",
            "Train_loss: 0.503896863362111 - train_acc: 92.30013106159898 - test_loss: 0.5035594602425894 - test_acc: 94.17989417989418\n",
            " ------------------Epoch: [53/100]-----------------\n",
            "Train_loss: 0.49981852463625986 - train_acc: 93.21756225425953 - test_loss: 0.5111055881888779 - test_acc: 63.49206349206349\n",
            " ------------------Epoch: [54/100]-----------------\n",
            "Train_loss: 0.5119105916504466 - train_acc: 91.71035386631716 - test_loss: 0.5029018697915254 - test_acc: 95.10582010582011\n",
            " ------------------Epoch: [55/100]-----------------\n",
            "Train_loss: 0.5129338584908651 - train_acc: 91.44823066841415 - test_loss: 0.5111044402475711 - test_acc: 93.78306878306877\n",
            " ------------------Epoch: [56/100]-----------------\n",
            "Train_loss: 0.5130455351750786 - train_acc: 91.6120576671035 - test_loss: 0.5029018697915254 - test_acc: 95.10582010582011\n",
            " ------------------Epoch: [57/100]-----------------\n",
            "Train_loss: 0.5138710772772448 - train_acc: 91.28440366972482 - test_loss: 0.5029018697915254 - test_acc: 95.10582010582011\n",
            " ------------------Epoch: [58/100]-----------------\n",
            "Train_loss: 0.5101424133012055 - train_acc: 92.00524246395807 - test_loss: 0.5029018697915254 - test_acc: 95.10582010582011\n",
            " ------------------Epoch: [59/100]-----------------\n",
            "Train_loss: 0.5105937536156505 - train_acc: 91.9724770642202 - test_loss: 0.5111044402475711 - test_acc: 93.78306878306877\n",
            " ------------------Epoch: [60/100]-----------------\n",
            "Train_loss: 0.5206823152139646 - train_acc: 90.49803407601577 - test_loss: 0.5266893329443755 - test_acc: 91.26984126984127\n",
            " ------------------Epoch: [61/100]-----------------\n",
            "Train_loss: 0.5072902334392617 - train_acc: 92.5294888597641 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [62/100]-----------------\n",
            "Train_loss: 0.5064521579567446 - train_acc: 92.56225425950198 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [63/100]-----------------\n",
            "Train_loss: 0.5034245635391376 - train_acc: 93.18479685452161 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [64/100]-----------------\n",
            "Train_loss: 0.5065510221030733 - train_acc: 92.66055045871562 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [65/100]-----------------\n",
            "Train_loss: 0.5073765598305868 - train_acc: 92.52948885976406 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [66/100]-----------------\n",
            "Train_loss: 0.5067670118371281 - train_acc: 92.59501965923984 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [67/100]-----------------\n",
            "Train_loss: 0.5081435354477769 - train_acc: 92.36566186107464 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [68/100]-----------------\n",
            "Train_loss: 0.504846841096878 - train_acc: 92.9554390563565 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [69/100]-----------------\n",
            "Train_loss: 0.5030181957494229 - train_acc: 93.18479685452164 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [70/100]-----------------\n",
            "Train_loss: 0.5054893520993924 - train_acc: 92.85714285714282 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [71/100]-----------------\n",
            "Train_loss: 0.5065180678433234 - train_acc: 92.62778505897775 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [72/100]-----------------\n",
            "Train_loss: 0.5029138720363652 - train_acc: 93.34862385321102 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [73/100]-----------------\n",
            "Train_loss: 0.5045191875291527 - train_acc: 93.08650065530799 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [74/100]-----------------\n",
            "Train_loss: 0.5051744954848508 - train_acc: 92.9554390563565 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [75/100]-----------------\n",
            "Train_loss: 0.5040039037345746 - train_acc: 93.15203145478371 - test_loss: 0.5193070151187755 - test_acc: 92.46031746031747\n",
            " ------------------Epoch: [76/100]-----------------\n",
            "Train_loss: 0.5205322312652518 - train_acc: 90.59633027522938 - test_loss: 0.543094480479205 - test_acc: 88.62433862433863\n",
            " ------------------Epoch: [77/100]-----------------\n",
            "Train_loss: 0.5276436395601395 - train_acc: 89.5478374836173 - test_loss: 0.5275095855748212 - test_acc: 91.13756613756614\n",
            " ------------------Epoch: [78/100]-----------------\n",
            "Train_loss: 0.5235543625617246 - train_acc: 90.26867627785062 - test_loss: 0.5275095855748212 - test_acc: 91.13756613756614\n",
            " ------------------Epoch: [79/100]-----------------\n",
            "Train_loss: 0.5245958870157189 - train_acc: 90.07208387942333 - test_loss: 0.5275095855748212 - test_acc: 91.13756613756614\n",
            " ------------------Epoch: [80/100]-----------------\n",
            "Train_loss: 0.516942557938602 - train_acc: 91.21887287024897 - test_loss: 0.5037221268371299 - test_acc: 94.97354497354497\n",
            " ------------------Epoch: [81/100]-----------------\n",
            "Train_loss: 0.49546013075277345 - train_acc: 94.39711664482304 - test_loss: 0.5201272633340623 - test_acc: 92.32804232804234\n",
            " ------------------Epoch: [82/100]-----------------\n",
            "Train_loss: 0.49249109926573725 - train_acc: 94.65923984272607 - test_loss: 0.5201272633340623 - test_acc: 92.32804232804234\n",
            " ------------------Epoch: [83/100]-----------------\n",
            "Train_loss: 0.48990281760145765 - train_acc: 95.31454783748363 - test_loss: 0.5119247017083345 - test_acc: 93.65079365079364\n",
            " ------------------Epoch: [84/100]-----------------\n",
            "Train_loss: 0.4877881801456486 - train_acc: 95.01965923984272 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [85/100]-----------------\n",
            "Train_loss: 0.48921080822244695 - train_acc: 94.72477064220188 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [86/100]-----------------\n",
            "Train_loss: 0.48506296993395603 - train_acc: 95.60943643512451 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [87/100]-----------------\n",
            "Train_loss: 0.4895768797178881 - train_acc: 94.42988204456093 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [88/100]-----------------\n",
            "Train_loss: 0.4914915826889353 - train_acc: 94.20052424639577 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [89/100]-----------------\n",
            "Train_loss: 0.4902395302549415 - train_acc: 94.42988204456097 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [90/100]-----------------\n",
            "Train_loss: 0.4912298340863044 - train_acc: 94.39711664482307 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [91/100]-----------------\n",
            "Train_loss: 0.4879129974667085 - train_acc: 94.95412844036697 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [92/100]-----------------\n",
            "Train_loss: 0.49033838647221206 - train_acc: 94.59370904325034 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [93/100]-----------------\n",
            "Train_loss: 0.48952565554085126 - train_acc: 94.69200524246392 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [94/100]-----------------\n",
            "Train_loss: 0.48663533581506224 - train_acc: 95.11795543905637 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [95/100]-----------------\n",
            "Train_loss: 0.48748101854543074 - train_acc: 95.0524246395806 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [96/100]-----------------\n",
            "Train_loss: 0.48887034676490576 - train_acc: 94.79030144167754 - test_loss: 0.4955195453431871 - test_acc: 96.29629629629629\n",
            " ------------------Epoch: [97/100]-----------------\n",
            "Train_loss: 0.4968214838876637 - train_acc: 94.26605504587154 - test_loss: 0.5119247017083345 - test_acc: 93.65079365079364\n",
            " ------------------Epoch: [98/100]-----------------\n",
            "Train_loss: 0.49774278194532484 - train_acc: 94.26605504587152 - test_loss: 0.5119247017083345 - test_acc: 93.65079365079364\n",
            " ------------------Epoch: [99/100]-----------------\n",
            "Train_loss: 0.5014384990438409 - train_acc: 93.51245085190038 - test_loss: 0.5037221268371299 - test_acc: 94.97354497354497\n",
            " ------------------Epoch: [100/100]-----------------\n",
            "Train_loss: 0.49442594450548155 - train_acc: 94.72477064220182 - test_loss: 0.5037221268371299 - test_acc: 94.97354497354497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fourtin = multimodal(2, 1).to(device)\n",
        "concat_optimizer = optim.Adam(fourtin.parameters(), lr=0.001)\n",
        "optimizer_2 = optim.Adam([{'params': cnn_model_4.parameters()},\n",
        "                        {'params': fourtin.parameters()}],\n",
        "                        lr=0.01)\n",
        "\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "results = {\n",
        "    \"train_loss\":[],\n",
        "    \"train_acc\":[],\n",
        "    \"test_loss\":[],\n",
        "    \"test_acc\":[]\n",
        "}\n",
        "\n",
        "# Training loop (adjust as needed)\n",
        "epochs = 100\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  cnn_model_4.train()\n",
        "  train_loss = 0.0\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  accuracy_per_batch = 0\n",
        "  num_batch_run = 0\n",
        "  total_concat_loss = 0\n",
        "\n",
        "  for batch in multimodal_train_dataloader:\n",
        "    train_acc = 0.0\n",
        "    total_acc = 0.0\n",
        "\n",
        "    images = batch['image']\n",
        "    texts = batch['text']\n",
        "    image_labels = batch['image_target']\n",
        "    text_labels= batch['text_target']\n",
        "\n",
        "    images, image_labels= images.to(device), image_labels.to(device)\n",
        "\n",
        "    texts, _ = texts\n",
        "    texts, text_labels= texts.to(device), text_labels.to(device)\n",
        "\n",
        "    # datatype of labels is int64 so we have to convert it to float32\n",
        "    image_labels = image_labels.type(torch.float32)\n",
        "    # forward pass\n",
        "    train_logit = cnn_model_4(images).squeeze()\n",
        "    # print(train_pred.shape)\n",
        "    train_pred = torch.round(torch.sigmoid(train_logit)).unsqueeze(dim = 1)\n",
        "\n",
        "    # Train an SVM classifier for text\n",
        "    svm_classifier_4.fit(texts.cpu(), text_labels.cpu())\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred_svm_QT = svm_classifier_4.predict(texts.cpu())\n",
        "    y_pred_svm_QT = torch.tensor(y_pred_svm_QT).unsqueeze(dim = 1)\n",
        "\n",
        "    combined_features = torch.cat((train_pred.to(device), y_pred_svm_QT.to(device)), dim=1)\n",
        "\n",
        "    combined_features = combined_features.unsqueeze(dim=0)\n",
        "\n",
        "    final_output = fourtin(combined_features.permute(1,2,0).to(device)).squeeze()\n",
        "    # print(final_output)\n",
        "        # Binary prediction (0 or 1) based on the sigmoid output\n",
        "    predicted = (final_output > 0.5).float()\n",
        "\n",
        "    predicted = predicted.requires_grad_(True)\n",
        "\n",
        "\n",
        "    # calculate the loss\n",
        "    concat_loss = loss_fn(final_output,image_labels)\n",
        "    train_loss += concat_loss.item()\n",
        "\n",
        "\n",
        "    # calculate accuracy\n",
        "    acc = (((predicted == image_labels).sum().item() / len(image_labels)) * 100 )\n",
        "    train_acc += acc\n",
        "\n",
        "    # optimizer zero_grad\n",
        "    concat_optimizer.zero_grad()\n",
        "\n",
        "    # loss backward\n",
        "    concat_loss.backward()\n",
        "\n",
        "    # optimizer step\n",
        "    concat_optimizer.step()\n",
        "\n",
        "  train_loss /= len(multimodal_train_dataloader)\n",
        "  train_acc /= len(multimodal_train_dataloader)\n",
        "                                  #############\n",
        "###################################  testing  ###########################################\n",
        "                                  #############\n",
        "\n",
        "  test_loss, test_acc = 0, 0\n",
        "  fourtin.eval()\n",
        "  with torch.inference_mode():\n",
        "    for batch in multimodal_test_dataloader:\n",
        "    # Pass image and text inputs through the multi_model\n",
        "\n",
        "      images = batch['image']\n",
        "      texts = batch['text']\n",
        "      image_labels = batch['image_target']\n",
        "      text_labels= batch['text_target']\n",
        "\n",
        "      images, image_labels= images.to(device), image_labels.to(device)\n",
        "\n",
        "      texts, _ = texts\n",
        "      texts, text_labels= texts.to(device), text_labels.to(device)\n",
        "\n",
        "      # datatype of labels is int64 so we have to convert it to float32\n",
        "      image_labels = image_labels.type(torch.float32)\n",
        "\n",
        "      # forward pass\n",
        "      test_logits = cnn_model_4(images).squeeze()\n",
        "      # print(train_pred.shape)\n",
        "      test_pred = torch.round(torch.sigmoid(test_logits)).unsqueeze(dim = 1) #unsqueezed it so that i can concat in dim = 1\n",
        "\n",
        "      # Make predictions\n",
        "      y_pred_svm_QT = svm_classifier_4.predict(texts.cpu())\n",
        "      y_pred_svm_QT = torch.tensor(y_pred_svm_QT).unsqueeze(dim = 1) #unsqueezed it so that i can concat in dim = 1\n",
        "\n",
        "      combined_features = torch.cat((train_pred.to(device), y_pred_svm_QT.to(device)), dim=1)\n",
        "\n",
        "      combined_features = combined_features.unsqueeze(dim=0) # unsqueezed it because myshape was [32,2] where it should be [32,2,1] so unsqueezed it permuted it.\n",
        "\n",
        "      test_logit = fourtin(combined_features.permute(1,2,0).to(device)).squeeze()\n",
        "\n",
        "      test_pred = torch.round(torch.sigmoid(test_logit))\n",
        "\n",
        "      # calculate the loss\n",
        "      loss = loss_fn(test_logit, image_labels)\n",
        "      test_loss += loss.item()\n",
        "\n",
        "      # calculate the accuracy\n",
        "      acc = (((test_pred == image_labels).sum().item() / len(test_pred)) * 100)\n",
        "      test_acc += acc\n",
        "\n",
        "    test_loss /= len(multimodal_test_dataloader)\n",
        "    test_acc /= len(multimodal_test_dataloader)\n",
        "\n",
        "  results[\"train_loss\"].append(train_loss)\n",
        "  results[\"train_acc\"].append(train_acc)\n",
        "  results[\"test_loss\"].append(test_loss)\n",
        "  results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "\n",
        "  print(f' ------------------Epoch: [{epoch+1}/{epochs}]-----------------')\n",
        "  print(f'Train_loss: {train_loss} - train_acc: {train_acc} - test_loss: {test_loss} - test_acc: {test_acc}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ggBhU6J2m_Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## apatotho off raki"
      ],
      "metadata": {
        "id": "oGAjLPikfdm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the CNN for image processing\n",
        "num_epochs = 50\n",
        "\n",
        "optimizer = optim.Adam(cnn_model_4.parameters(), lr=0.001)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Define a function to train the image CNN\n",
        "# def train_cnn():\n",
        "num = 0\n",
        "flag = False\n",
        "# Training loop for CNN\n",
        "for epoch in range(num_epochs):\n",
        "  cnn_model_4.train()\n",
        "  train_loss = 0.0\n",
        "  train_acc = 0.0\n",
        "  total_acc = 0.0\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  accuracy_per_batch = 0\n",
        "  num_batch_run = 0\n",
        "\n",
        "\n",
        "\n",
        "  for (images, image_labels) in train_dataloader:\n",
        "    predicted = []\n",
        "    total_batches = 0\n",
        "    images, image_labels= images.to(device), image_labels.to(device)\n",
        "    # datatype of labels is int64 so we have to convert it to float32\n",
        "    image_labels = image_labels.type(torch.float32)\n",
        "    # forward pass\n",
        "    train_logit = cnn_model_4(images).squeeze()\n",
        "    train_pred = torch.round(torch.sigmoid(train_logit))\n",
        "\n",
        "    # calculate the loss\n",
        "    loss = loss_fn(train_logit,image_labels)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    # calculate accuracy\n",
        "    acc = (((train_pred == image_labels).sum().item() / len(train_pred)) * 100 )\n",
        "    train_acc += acc\n",
        "\n",
        "    # optimizer zero_grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # for batch, (text, labels) in enumerate(train_text_dataloader):\n",
        "    combined_features = []\n",
        "\n",
        "\n",
        "    # # Assuming labelsou have text labels (replace with your labels)\n",
        "    # y = [...]  # List of corresponding labels\n",
        "\n",
        "    # Process the corresponding text batch\n",
        "    text, labels = next(iter(train_text_dataloader))\n",
        "\n",
        "    # Train an SVM classifier for text\n",
        "    svm_classifier_4.fit(text, labels)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred_svm_QT = svm_classifier_4.predict(text)\n",
        "    y_pred_svm_QT = torch.tensor(y_pred_svm_QT)\n",
        "\n",
        "    # Calculate accuracy and print classification report\n",
        "    # accuracy_svm_QT = accuracy_score(labels, y_pred_svm_QT)\n",
        "    # print(y_pred_svm_QT)\n",
        "\n",
        "    train_pred = train_pred.detach().cpu()\n",
        "    # print(train_pred)\n",
        "\n",
        "    for i in range(len(train_pred)):\n",
        "      # print(i)\n",
        "      combined_features.append(torch.cat((train_pred[i].unsqueeze(dim=0),y_pred_svm_QT[i].unsqueeze(dim=0)),dim=0)) #.unsqueeze(dim=0)\n",
        "      # print(cat.dtype)\n",
        "      # combined_features.append(cat)\n",
        "\n",
        "    print(combined_features)\n",
        "    # combined_features = torch.stack(combined_features)\n",
        "    combined_features = torch.stack(combined_features)\n",
        "    # print(combined_features.dtype)\n",
        "    # print(\"SVM Accuracy QT:\", accuracy_svm_QT)\n",
        "\n",
        "    # combined_features = torch.cat((train_pred, torch.tensor(y_pred_svm_QT).to(device)), dim=0)\n",
        "\n",
        "    # # The concatenated_scores now contain the combined scores\n",
        "    # print(\"Concatenated Scores:\")\n",
        "    # print(concatenated_scores)\n",
        "\n",
        "    # print(f\"combine of them is: {combined_features}\")\n",
        "    for i in range(combined_features.shape[0]):\n",
        "      final_output = fourtin(combined_features[i].to(device))\n",
        "        # Binary prediction (0 or 1) based on the sigmoid output\n",
        "      # predicted.append((final_output > 0.5).float())\n",
        "      # predicted.append(torch.round(torch.sigmoid(final_output)))\n",
        "      predicted.append(final_output.float())\n",
        "    # print(predicted)\n",
        "    # print(num)\n",
        "    # num = num + 1\n",
        "    predicted = torch.cat(predicted)\n",
        "    print(predicted)\n",
        "\n",
        "    total = len(image_labels)\n",
        "    correct = (predicted.squeeze() == image_labels).sum().item()\n",
        "\n",
        "    acc = ((correct / total) * 100)\n",
        "    print(acc)\n",
        "    total_acc += acc\n",
        "    num_batch_run += 1\n",
        "\n",
        "  print(num_batch_run)\n",
        "  print(len(train_dataloader))\n",
        "  total_acc = total_acc / len(train_dataloader)\n",
        "  print(\"==============================\")\n",
        "  print(total_acc)\n",
        "  #     total_batches += 1\n",
        "  # total_accuracy += accuracy_per_text_batch / total_batches\n",
        "  # print(\"==============================\")\n",
        "  # print(f\"Multimodal Model Accuracy: {total_accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "  # avg_loss = train_loss / len(train_dataloader)\n",
        "  # avg_acc = train_acc / len(train_dataloader)\n",
        "  # print(f\"Image CNN - Epoch [{epoch+1}/{num_epochs}] Loss: {avg_loss:.4f} acc: {avg_acc}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vtoxMnfav43F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52641d53-74e5-4f4b-ac2d-3fe3699a66cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([1., 1.]), tensor([1., 1.]), tensor([1., 0.]), tensor([1., 0.]), tensor([1., 0.]), tensor([1., 1.]), tensor([1., 0.]), tensor([1., 0.]), tensor([1., 1.]), tensor([1., 0.]), tensor([1., 0.]), tensor([1., 1.]), tensor([1., 0.]), tensor([1., 1.]), tensor([1., 1.]), tensor([1., 0.]), tensor([1., 1.]), tensor([1., 1.]), tensor([1., 0.]), tensor([1., 1.]), tensor([1., 1.]), tensor([1., 0.]), tensor([1., 0.]), tensor([1., 0.]), tensor([1., 0.]), tensor([1., 0.]), tensor([1., 0.]), tensor([1., 1.]), tensor([1., 0.]), tensor([1., 1.])]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-124-a81604829394>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# print(f\"combine of them is: {combined_features}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m       \u001b[0mfinal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfourtin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Binary prediction (0 or 1) based on the sigmoid output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0;31m# predicted.append((final_output > 0.5).float())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-110-65ea06e73126>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m       \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify_fusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    307\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m--> 309\u001b[0;31m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    310\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [2]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "path = \"/content/final_80_20_124res/train/ASD\"\n",
        "path2 = \"/content/final_80_20_124res/train/TD\"\n",
        "j = 0\n",
        "k = 0\n",
        "for i in os.listdir(path):\n",
        "  j = j+1\n",
        "print(j)\n",
        "for i in os.listdir(path2):\n",
        "  k = k+1\n",
        "print(k)\n",
        "a = j+k\n",
        "print(a)\n",
        "\n",
        "a = a/32\n",
        "a\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R59-b7tnlbpp",
        "outputId": "1d1fd42d-be14-4d3f-abcd-30e34854ba04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1216\n",
            "1847\n",
            "3063\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95.71875"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming you have image tensors (image_data_tensor) and text tensors (text_data_tensor) loaded and preprocessed\n",
        "\n",
        "# Create DataLoader instances for both image and text data (batch size 32)\n",
        "image_batch_size = 32\n",
        "text_batch_size = 32  # Adjust this based on your text dataset size\n",
        "\n",
        "image_dataset = TensorDataset(image_data_tensor)\n",
        "image_data_loader = DataLoader(image_dataset, batch_size=image_batch_size, shuffle=True)\n",
        "\n",
        "text_dataset = TensorDataset(text_data_tensor)\n",
        "text_data_loader = DataLoader(text_dataset, batch_size=text_batch_size, shuffle=True)\n",
        "\n",
        "# Define and load a pre-trained image model (e.g., ResNet)\n",
        "image_model = models.resnet18(pretrained=True)\n",
        "image_model.fc = nn.Linear(image_model.fc.in_features, 2)  # 2 classes for binary classification\n",
        "\n",
        "# Define a TF-IDF vectorizer for text data\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features as needed\n",
        "\n",
        "# Load and preprocess text data (e.g., tokenization, TF-IDF conversion)\n",
        "text_samples = [...]  # List of text samples\n",
        "text_labels = [...]   # Corresponding labels (0 or 1)\n",
        "\n",
        "# Train an SVM classifier on the text data\n",
        "text_classifier = SVC(kernel='linear', C=1.0)\n",
        "\n",
        "# Define your custom multimodal late fusion classifier\n",
        "class MultimodalClassifier(nn.Module):\n",
        "    def __init__(self, image_model, text_classifier):\n",
        "        super(MultimodalClassifier, self).__init__()\n",
        "        self.image_model = image_model\n",
        "        self.text_classifier = text_classifier\n",
        "        self.fc = nn.Linear(image_model.fc.out_features + 2, 2)  # Combine image features with text SVM's output\n",
        "\n",
        "    def forward(self, image_batch, text_batch):\n",
        "        # Pass the image batch through the image model to get predictions\n",
        "        image_batch_predictions = self.image_model(image_batch)\n",
        "\n",
        "        # Process the text batch using the text classifier\n",
        "        text_batch_predictions = self.text_classifier.predict(text_batch)\n",
        "\n",
        "        # Combine the predictions from both modalities\n",
        "        combined_features = torch.cat((image_batch_predictions, torch.tensor(text_batch_predictions).reshape(-1, 1)), dim=1)\n",
        "\n",
        "        # Pass the combined features through the classification layer\n",
        "        logits = self.fc(combined_features)\n",
        "        return logits\n",
        "\n",
        "# Instantiate your custom multimodal classifier\n",
        "multimodal_classifier = MultimodalClassifier(image_model, text_classifier)\n",
        "\n",
        "# Define a loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(multimodal_classifier.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10  # Adjust the number of epochs as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for image_batch, text_batch in zip(image_data_loader, text_data_loader):\n",
        "        # Assuming image_data_loader and text_data_loader are DataLoader instances\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        logits = multimodal_classifier(image_batch, text_batch)\n",
        "\n",
        "        # Compute the loss\n",
        "        labels = torch.tensor(text_labels, dtype=torch.long)  # Assuming text_labels are integers (0 or 1)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluation\n",
        "multimodal_classifier.eval()\n",
        "combined_predictions = []\n",
        "\n",
        "for image_batch, text_batch in zip(image_data_loader, text_data_loader):\n",
        "    # Assuming image_data_loader and text_data_loader are DataLoader instances\n",
        "    logits = multimodal_classifier(image_batch, text_batch)\n",
        "    _, predicted = torch.max(logits.data, 1)\n",
        "    combined_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Evaluate the multimodal classifier on your test dataset (ground truth labels needed)\n",
        "# You can use metrics like accuracy_score for evaluation\n",
        "ground_truth_labels = [...]  # Replace with actual ground truth labels\n",
        "accuracy = accuracy_score(ground_truth_labels, combined_predictions)\n",
        "\n",
        "print(f\"Multimodal Classifier Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "AREPPCke0IFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## weighted fusion"
      ],
      "metadata": {
        "id": "qwgqYIwvS0Ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define separate models for each modality\n",
        "class TextModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(TextModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        x = torch.relu(self.fc1(text))\n",
        "        return self.fc2(x)\n",
        "\n",
        "class ImageModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ImageModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, image):\n",
        "        x = torch.relu(self.fc1(image))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# Define the fusion layer with learned weights\n",
        "class WeightedFusion(nn.Module):\n",
        "    def __init__(self, num_modalities):\n",
        "        super(WeightedFusion, self).__init__()\n",
        "        self.weights = nn.Parameter(torch.ones(num_modalities, requires_grad=True))\n",
        "\n",
        "    def forward(self, modality_outputs):\n",
        "        weighted_sum = torch.sum(self.weights * modality_outputs, dim=0)\n",
        "        return weighted_sum\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Define hyperparameters\n",
        "    NUM_EPOCHS = 100\n",
        "    TEXT_INPUT_DIM = 300  # Dimensionality of text data\n",
        "    IMAGE_INPUT_DIM = 2048  # Dimensionality of image data\n",
        "    HIDDEN_DIM = 128\n",
        "    OUTPUT_DIM = 1  # Binary classification task\n",
        "\n",
        "    # Initialize models for each modality\n",
        "    text_model = TextModel(79, HIDDEN_DIM, OUTPUT_DIM)\n",
        "    image_model = ImageModel(3, HIDDEN_DIM, OUTPUT_DIM)\n",
        "\n",
        "    # Initialize fusion layer\n",
        "    fusion_layer = WeightedFusion(num_modalities=2)  # Two modalities: text and image\n",
        "\n",
        "    # Define optimizer and loss function\n",
        "    optimizer = optim.Adam([{'params': text_model.parameters()},\n",
        "                            {'params': image_model.parameters()},\n",
        "                            {'params': fusion_layer.parameters()}])\n",
        "    loss_fn = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss for binary classification\n",
        "\n",
        "    # Training loop (adapt this loop based on your dataset)\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        for batch in multimodal_train_dataloader:\n",
        "            images = batch['image']\n",
        "            texts = batch['text']\n",
        "            image_labels = batch['image_target']\n",
        "            text_labels= batch['text_target']\n",
        "\n",
        "            images, image_labels= images.to(device), image_labels.to(device)\n",
        "\n",
        "            texts, _ = texts\n",
        "            texts, text_labels= texts.to(device), text_labels.to(device)\n",
        "\n",
        "            text_output = text_model(texts)\n",
        "            image_output = image_model(images)\n",
        "\n",
        "            # Combine modality-specific outputs using weighted fusion\n",
        "            modality_outputs = torch.stack([text_output, image_output], dim=0)\n",
        "            fused_output = fusion_layer(modality_outputs)\n",
        "\n",
        "            loss = loss_fn(fused_output, labels.float())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n"
      ],
      "metadata": {
        "id": "BokObHs7SzaB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}